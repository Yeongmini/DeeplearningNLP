{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7636af9",
   "metadata": {},
   "source": [
    "8-7 문자 단위 RNN(Char RNN)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a65f70da",
   "metadata": {},
   "source": [
    "지금까지 배운 RNN은 전부 입력과 출력의 단위가 단어 벡터였다. but 입출력의 단위를 단어 레벨에서 문자 레벨로 변경하여 RNN을 구현할 수 있다.\\\n",
    "![image.png](attachment:image.png)\\\n",
    "문자단위 RNN을 다 대 다 구조로 구현한 경우, 다 대 일 구조로 구현한 경우 두 가지를 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81fe80",
   "metadata": {},
   "source": [
    "1. 문자단위 RNN 언어 모델(Char RNNLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01200cc",
   "metadata": {},
   "source": [
    "이전 시점의 예측 문자를 다음 시점의 입력으로 사용하는 문자 단위 RNN언어 모델을 구현해보자.\\\n",
    "앞서 배운 단어 단위 RNN 언어 모델과 다른점은 단어 단위가 아니라 문자 단위를 입,출력으로 사용하므로 임베딩층을 여기서는 사용하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201df89b",
   "metadata": {},
   "source": [
    "1) 데이터에 대한 이해와 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb48ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 데이터 로드\n",
    "urllib.request.urlretrieve(\"http://www.gutenberg.org/files/11/11-0.txt\", filename=\"11-0.txt\")\n",
    "\n",
    "f = open('11-0.txt', 'rb')\n",
    "sentences = []\n",
    "for sentence in f: # 데이터로부터 한 줄씩 읽는다.\n",
    "    sentence = sentence.strip() # strip()을 통해 \\r, \\n을 제거한다.\n",
    "    sentence = sentence.lower() # 소문자화.\n",
    "    sentence = sentence.decode('ascii', 'ignore') # \\xe2\\x80\\x99 등과 같은 바이트 열 제거\n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad96968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the project gutenberg ebook of alices adventures in wonderland, by lewis carroll',\n",
       " 'this ebook is for the use of anyone anywhere in the united states and',\n",
       " 'most other parts of the world at no cost and with almost no restrictions',\n",
       " 'whatsoever. you may copy it, give it away or re-use it under the terms',\n",
       " 'of the project gutenberg license included with this ebook or online at']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69170c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자열의 길이 또는 총 문자의 개수 : 159484\n"
     ]
    }
   ],
   "source": [
    "# 하나의 문자열로 통합\n",
    "total_data = ' '.join(sentences)\n",
    "print('문자열의 길이 또는 총 문자의 개수 : %d' % len(total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cddecc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the project gutenberg ebook of alices adventures in wonderland, by lewis carroll this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with\n"
     ]
    }
   ],
   "source": [
    "print(total_data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38821140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 56\n"
     ]
    }
   ],
   "source": [
    "char_vocab = sorted(list(set(total_data)))\n",
    "vocab_size = len(char_vocab)\n",
    "print('문자 집합의 크기 : {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba30757",
   "metadata": {},
   "source": [
    "영어가 훈련 데이터일 때 문자 집합의 크기는 단어 집합을 사용했을 경우보다 집합의 크기가 현저히 작다.\\\n",
    "영어 단어를 표현하기 위해서 사용되는 문자는 26개의 알파벳뿐이기 때문이다.\\\n",
    "이처럼 어떤 방대한 양의 텍스트라도 집합의 크기를 적게 가져갈 수 있다는 것은 구현과 테스트에 큰 이점을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3c0ff74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합 : {' ': 0, '!': 1, '\"': 2, '#': 3, '$': 4, '%': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '?': 26, '[': 27, ']': 28, '_': 29, 'a': 30, 'b': 31, 'c': 32, 'd': 33, 'e': 34, 'f': 35, 'g': 36, 'h': 37, 'i': 38, 'j': 39, 'k': 40, 'l': 41, 'm': 42, 'n': 43, 'o': 44, 'p': 45, 'q': 46, 'r': 47, 's': 48, 't': 49, 'u': 50, 'v': 51, 'w': 52, 'x': 53, 'y': 54, 'z': 55}\n"
     ]
    }
   ],
   "source": [
    "# 문자에 고유한 정수 부여\n",
    "char_to_index = dict((char, index) for index, char in enumerate(char_vocab))\n",
    "print('문자 집합 :',char_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f264276",
   "metadata": {},
   "source": [
    "0부터 28 까지는 공백을 포함한 각종 구두점, 특수문자가 존재 , 29부터 54 까지는 a부터 z까지 총 26개의 알파벳 소문자가 문자 집합에 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3decf91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수로 부터 문자를 리턴하는 index_to_char\n",
    "index_to_char = {}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6dd294",
   "metadata": {},
   "source": [
    "if 훈련 데이터에 apple이라는 시퀀스가 있고, 입력의 길이를 4라고 정하였을 때 입력 시퀀스와 출력 시퀀스 모두 길이는 4가 된다.\\\n",
    "다시 말해 RNN은 총 네 번의 시점(timestep)을 가질 수 있다는 의미이다. apple은 다섯 글자이지만 입력의 길이는 4이므로 'appl'까지만 입력으로 사용 가능하다.\\\n",
    "그리고 언어 모델은 다음 시점의 입력을 예측해야하는 모델이므로 'pple'를 예측하도록 데이터가 구성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ed9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appl (입력 시퀀스) -> pple (출력 시퀀스)\n",
    "train_X = 'appl'\n",
    "train_y = 'pple'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458bbb5f",
   "metadata": {},
   "source": [
    "데이터를 만드는 방법은 문장 샘플의 길이를 정하고, 해당 길이만큼 문자열 전체를 등분하는 것이다.\\\n",
    "여기서는 문장의 길이를 60으로 정하였는데, 즉 15만9천을 60으로 나눈 수가 샘플의 수가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "524febdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 수 : 2658\n"
     ]
    }
   ],
   "source": [
    "seq_length = 60\n",
    "\n",
    "# 문자열의 길이를 seq_length로 나누면 전처리 후 생겨날 샘플 수\n",
    "n_samples = int(np.floor((len(total_data) - 1) / seq_length))\n",
    "print('샘플의 수 : {}'.format(n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b422ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "train_y = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # 0:60 -> 60:120 -> 120:180으로 loop를 돌면서 문장 샘플을 1개씩 pick\n",
    "    X_sample = total_data[i * seq_length : (i+1) * seq_length]\n",
    "    \n",
    "    # 정수 인코딩\n",
    "    X_encoded = [char_to_index[c] for c in X_sample]\n",
    "    train_X.append(X_encoded)\n",
    "    \n",
    "    # 오른쪽으로 1칸 쉬프트\n",
    "    y_sample = total_data[i * seq_length + 1 : (i+1) * seq_length + 1]\n",
    "    y_encoded = [char_to_index[c] for c in y_sample]\n",
    "    train_y.append(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1987731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 데이터의 첫번째 샘플 : [49, 37, 34, 0, 45, 47, 44, 39, 34, 32, 49, 0, 36, 50, 49, 34, 43, 31, 34, 47, 36, 0, 34, 31, 44, 44, 40, 0, 44, 35, 0, 30, 41, 38, 32, 34, 48, 0, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 0, 38, 43, 0, 52, 44, 43, 33, 34, 47, 41, 30]\n",
      "y 데이터의 첫번째 샘플 : [37, 34, 0, 45, 47, 44, 39, 34, 32, 49, 0, 36, 50, 49, 34, 43, 31, 34, 47, 36, 0, 34, 31, 44, 44, 40, 0, 44, 35, 0, 30, 41, 38, 32, 34, 48, 0, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 0, 38, 43, 0, 52, 44, 43, 33, 34, 47, 41, 30, 43]\n",
      "--------------------------------------------------\n",
      "X 데이터의 첫번째 샘플 디코딩 : ['t', 'h', 'e', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'b', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'l', 'a']\n",
      "y 데이터의 첫번째 샘플 디코딩 : ['h', 'e', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'b', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'l', 'a', 'n']\n"
     ]
    }
   ],
   "source": [
    "print('X 데이터의 첫번째 샘플 :',train_X[0])\n",
    "print('y 데이터의 첫번째 샘플 :',train_y[0])\n",
    "print('-'*50)\n",
    "print('X 데이터의 첫번째 샘플 디코딩 :',[index_to_char[i] for i in train_X[0]])\n",
    "print('y 데이터의 첫번째 샘플 디코딩 :',[index_to_char[i] for i in train_y[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97600c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 33, 10, 0, 31, 54, 0, 41, 34, 52, 38, 48, 0, 32, 30, 47, 47, 44, 41, 41, 0, 49, 37, 38, 48, 0, 34, 31, 44, 44, 40, 0, 38, 48, 0, 35, 44, 47, 0, 49, 37, 34, 0, 50, 48, 34, 0, 44, 35, 0, 30, 43, 54, 44, 43, 34, 0, 30, 43, 54]\n",
      "[33, 10, 0, 31, 54, 0, 41, 34, 52, 38, 48, 0, 32, 30, 47, 47, 44, 41, 41, 0, 49, 37, 38, 48, 0, 34, 31, 44, 44, 40, 0, 38, 48, 0, 35, 44, 47, 0, 49, 37, 34, 0, 50, 48, 34, 0, 44, 35, 0, 30, 43, 54, 44, 43, 34, 0, 30, 43, 54, 52]\n"
     ]
    }
   ],
   "source": [
    "print(train_X[1])\n",
    "print(train_y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf2bb3",
   "metadata": {},
   "source": [
    "train_X와 train_y에 대해서 원-핫 인코딩을 수행한다. 문자 단위 RNN에서는 입력 시퀀스에 대해서 워드 임베딩을 하지 않는다.\\\n",
    "즉, 임베딩층을 사용하지 않으므로 입력 시퀀스인 train_X에 대해서도 원-핫 인코딩을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1a19595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X의 크기(shape) : (2658, 60, 56)\n",
      "train_y의 크기(shape) : (2658, 60, 56)\n"
     ]
    }
   ],
   "source": [
    "train_X = to_categorical(train_X)\n",
    "train_y = to_categorical(train_y)\n",
    "\n",
    "print('train_X의 크기(shape) : {}'.format(train_X.shape)) # 원-핫 인코딩\n",
    "print('train_y의 크기(shape) : {}'.format(train_y.shape))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "1a07f839",
   "metadata": {},
   "source": [
    "train_X와 train_y의 크기는 2,658 x 60 x 56이다.\\\n",
    "![image.png](attachment:image.png)\\\n",
    "이는 샘플의 수(No. of samples)가 2,658개 , 입력 시퀀스의 길이(input_length)가 60, 각 벡터의 차원(input_dim)이 55임을 의미한다.\\\n",
    "원-핫 벡터의 차원은 문자 집합의 크기인 56이어야 하므로 원-핫 인코딩이 수행되었음을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf7904",
   "metadata": {},
   "source": [
    "2) 모델 설계하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2032115c",
   "metadata": {},
   "source": [
    "하이퍼파라미터인 은닉상태의 크기는 256, 모델은 다 대 다 구조의 LSTM을 사용, LSTM 은닉층은 두 개를 사용한다.\\\n",
    "전결합층을 출력층으로 문자 집합 크기만큼의 뉴런을 배치하여 모델을 설계한다. 해당 모델은 모든 시점에서 모든 가능한 문자 중 하나의 문자를 예측하는 다중 클래스 분류 문제를 수행하는 모델이다.\\\n",
    "출력층에 소프트맥스 회귀를 사용해야 하므로 활성화 함수로는 소프트맥스 함수, 손실 함수로는 크로스 엔트로피 함수를 사용하여 80에포크를 수행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d434a6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "84/84 - 24s - loss: 3.0698 - accuracy: 0.1827 - 24s/epoch - 282ms/step\n",
      "Epoch 2/80\n",
      "84/84 - 21s - loss: 2.7450 - accuracy: 0.2429 - 21s/epoch - 255ms/step\n",
      "Epoch 3/80\n",
      "84/84 - 21s - loss: 2.3836 - accuracy: 0.3313 - 21s/epoch - 251ms/step\n",
      "Epoch 4/80\n",
      "84/84 - 21s - loss: 2.2318 - accuracy: 0.3665 - 21s/epoch - 250ms/step\n",
      "Epoch 5/80\n",
      "84/84 - 22s - loss: 2.1203 - accuracy: 0.3928 - 22s/epoch - 256ms/step\n",
      "Epoch 6/80\n",
      "84/84 - 21s - loss: 2.0373 - accuracy: 0.4143 - 21s/epoch - 252ms/step\n",
      "Epoch 7/80\n",
      "84/84 - 21s - loss: 1.9693 - accuracy: 0.4311 - 21s/epoch - 253ms/step\n",
      "Epoch 8/80\n",
      "84/84 - 22s - loss: 1.9041 - accuracy: 0.4482 - 22s/epoch - 257ms/step\n",
      "Epoch 9/80\n",
      "84/84 - 21s - loss: 1.8474 - accuracy: 0.4648 - 21s/epoch - 254ms/step\n",
      "Epoch 10/80\n",
      "84/84 - 21s - loss: 1.7990 - accuracy: 0.4780 - 21s/epoch - 253ms/step\n",
      "Epoch 11/80\n",
      "84/84 - 22s - loss: 1.7525 - accuracy: 0.4897 - 22s/epoch - 266ms/step\n",
      "Epoch 12/80\n",
      "84/84 - 22s - loss: 1.7083 - accuracy: 0.5019 - 22s/epoch - 263ms/step\n",
      "Epoch 13/80\n",
      "84/84 - 22s - loss: 1.6706 - accuracy: 0.5108 - 22s/epoch - 260ms/step\n",
      "Epoch 14/80\n",
      "84/84 - 21s - loss: 1.6345 - accuracy: 0.5198 - 21s/epoch - 252ms/step\n",
      "Epoch 15/80\n",
      "84/84 - 22s - loss: 1.6011 - accuracy: 0.5286 - 22s/epoch - 263ms/step\n",
      "Epoch 16/80\n",
      "84/84 - 22s - loss: 1.5664 - accuracy: 0.5381 - 22s/epoch - 266ms/step\n",
      "Epoch 17/80\n",
      "84/84 - 22s - loss: 1.5331 - accuracy: 0.5468 - 22s/epoch - 267ms/step\n",
      "Epoch 18/80\n",
      "84/84 - 22s - loss: 1.5031 - accuracy: 0.5559 - 22s/epoch - 267ms/step\n",
      "Epoch 19/80\n",
      "84/84 - 21s - loss: 1.4736 - accuracy: 0.5633 - 21s/epoch - 251ms/step\n",
      "Epoch 20/80\n",
      "84/84 - 21s - loss: 1.4410 - accuracy: 0.5726 - 21s/epoch - 252ms/step\n",
      "Epoch 21/80\n",
      "84/84 - 21s - loss: 1.4122 - accuracy: 0.5816 - 21s/epoch - 251ms/step\n",
      "Epoch 22/80\n",
      "84/84 - 22s - loss: 1.3839 - accuracy: 0.5889 - 22s/epoch - 262ms/step\n",
      "Epoch 23/80\n",
      "84/84 - 21s - loss: 1.3528 - accuracy: 0.5979 - 21s/epoch - 252ms/step\n",
      "Epoch 24/80\n",
      "84/84 - 21s - loss: 1.3272 - accuracy: 0.6052 - 21s/epoch - 251ms/step\n",
      "Epoch 25/80\n",
      "84/84 - 21s - loss: 1.2990 - accuracy: 0.6136 - 21s/epoch - 248ms/step\n",
      "Epoch 26/80\n",
      "84/84 - 23s - loss: 1.2706 - accuracy: 0.6206 - 23s/epoch - 272ms/step\n",
      "Epoch 27/80\n",
      "84/84 - 23s - loss: 1.2423 - accuracy: 0.6300 - 23s/epoch - 274ms/step\n",
      "Epoch 28/80\n",
      "84/84 - 21s - loss: 1.2151 - accuracy: 0.6377 - 21s/epoch - 250ms/step\n",
      "Epoch 29/80\n",
      "84/84 - 22s - loss: 1.1858 - accuracy: 0.6467 - 22s/epoch - 263ms/step\n",
      "Epoch 30/80\n",
      "84/84 - 23s - loss: 1.1607 - accuracy: 0.6535 - 23s/epoch - 270ms/step\n",
      "Epoch 31/80\n",
      "84/84 - 22s - loss: 1.1298 - accuracy: 0.6634 - 22s/epoch - 262ms/step\n",
      "Epoch 32/80\n",
      "84/84 - 22s - loss: 1.1009 - accuracy: 0.6711 - 22s/epoch - 263ms/step\n",
      "Epoch 33/80\n",
      "84/84 - 23s - loss: 1.0743 - accuracy: 0.6790 - 23s/epoch - 268ms/step\n",
      "Epoch 34/80\n",
      "84/84 - 22s - loss: 1.0462 - accuracy: 0.6886 - 22s/epoch - 264ms/step\n",
      "Epoch 35/80\n",
      "84/84 - 22s - loss: 1.0194 - accuracy: 0.6959 - 22s/epoch - 262ms/step\n",
      "Epoch 36/80\n",
      "84/84 - 22s - loss: 0.9857 - accuracy: 0.7064 - 22s/epoch - 264ms/step\n",
      "Epoch 37/80\n",
      "84/84 - 22s - loss: 0.9584 - accuracy: 0.7150 - 22s/epoch - 258ms/step\n",
      "Epoch 38/80\n",
      "84/84 - 22s - loss: 0.9332 - accuracy: 0.7223 - 22s/epoch - 258ms/step\n",
      "Epoch 39/80\n",
      "84/84 - 22s - loss: 0.9066 - accuracy: 0.7297 - 22s/epoch - 258ms/step\n",
      "Epoch 40/80\n",
      "84/84 - 22s - loss: 0.8765 - accuracy: 0.7403 - 22s/epoch - 257ms/step\n",
      "Epoch 41/80\n",
      "84/84 - 21s - loss: 0.8485 - accuracy: 0.7491 - 21s/epoch - 250ms/step\n",
      "Epoch 42/80\n",
      "84/84 - 22s - loss: 0.8280 - accuracy: 0.7546 - 22s/epoch - 257ms/step\n",
      "Epoch 43/80\n",
      "84/84 - 22s - loss: 0.8054 - accuracy: 0.7613 - 22s/epoch - 256ms/step\n",
      "Epoch 44/80\n",
      "84/84 - 21s - loss: 0.7697 - accuracy: 0.7726 - 21s/epoch - 256ms/step\n",
      "Epoch 45/80\n",
      "84/84 - 21s - loss: 0.7449 - accuracy: 0.7810 - 21s/epoch - 249ms/step\n",
      "Epoch 46/80\n",
      "84/84 - 22s - loss: 0.7223 - accuracy: 0.7883 - 22s/epoch - 259ms/step\n",
      "Epoch 47/80\n",
      "84/84 - 22s - loss: 0.6943 - accuracy: 0.7971 - 22s/epoch - 258ms/step\n",
      "Epoch 48/80\n",
      "84/84 - 21s - loss: 0.6874 - accuracy: 0.7981 - 21s/epoch - 254ms/step\n",
      "Epoch 49/80\n",
      "84/84 - 23s - loss: 0.6476 - accuracy: 0.8128 - 23s/epoch - 273ms/step\n",
      "Epoch 50/80\n",
      "84/84 - 23s - loss: 0.6173 - accuracy: 0.8232 - 23s/epoch - 274ms/step\n",
      "Epoch 51/80\n",
      "84/84 - 23s - loss: 0.5954 - accuracy: 0.8299 - 23s/epoch - 270ms/step\n",
      "Epoch 52/80\n",
      "84/84 - 22s - loss: 0.5858 - accuracy: 0.8318 - 22s/epoch - 261ms/step\n",
      "Epoch 53/80\n",
      "84/84 - 22s - loss: 0.5636 - accuracy: 0.8386 - 22s/epoch - 259ms/step\n",
      "Epoch 54/80\n",
      "84/84 - 22s - loss: 0.5323 - accuracy: 0.8496 - 22s/epoch - 267ms/step\n",
      "Epoch 55/80\n",
      "84/84 - 23s - loss: 0.5060 - accuracy: 0.8598 - 23s/epoch - 273ms/step\n",
      "Epoch 56/80\n",
      "84/84 - 22s - loss: 0.4925 - accuracy: 0.8628 - 22s/epoch - 264ms/step\n",
      "Epoch 57/80\n",
      "84/84 - 22s - loss: 0.4781 - accuracy: 0.8668 - 22s/epoch - 263ms/step\n",
      "Epoch 58/80\n",
      "84/84 - 23s - loss: 0.4610 - accuracy: 0.8717 - 23s/epoch - 272ms/step\n",
      "Epoch 59/80\n",
      "84/84 - 22s - loss: 0.4348 - accuracy: 0.8813 - 22s/epoch - 265ms/step\n",
      "Epoch 60/80\n",
      "84/84 - 22s - loss: 0.4224 - accuracy: 0.8855 - 22s/epoch - 265ms/step\n",
      "Epoch 61/80\n",
      "84/84 - 22s - loss: 0.4046 - accuracy: 0.8905 - 22s/epoch - 258ms/step\n",
      "Epoch 62/80\n",
      "84/84 - 22s - loss: 0.3988 - accuracy: 0.8912 - 22s/epoch - 258ms/step\n",
      "Epoch 63/80\n",
      "84/84 - 22s - loss: 0.3843 - accuracy: 0.8960 - 22s/epoch - 262ms/step\n",
      "Epoch 64/80\n",
      "84/84 - 22s - loss: 0.3538 - accuracy: 0.9074 - 22s/epoch - 259ms/step\n",
      "Epoch 65/80\n",
      "84/84 - 22s - loss: 0.3393 - accuracy: 0.9130 - 22s/epoch - 266ms/step\n",
      "Epoch 66/80\n",
      "84/84 - 23s - loss: 0.3246 - accuracy: 0.9170 - 23s/epoch - 268ms/step\n",
      "Epoch 67/80\n",
      "84/84 - 22s - loss: 0.3116 - accuracy: 0.9214 - 22s/epoch - 267ms/step\n",
      "Epoch 68/80\n",
      "84/84 - 22s - loss: 0.3025 - accuracy: 0.9234 - 22s/epoch - 263ms/step\n",
      "Epoch 69/80\n",
      "84/84 - 22s - loss: 0.3080 - accuracy: 0.9199 - 22s/epoch - 262ms/step\n",
      "Epoch 70/80\n",
      "84/84 - 21s - loss: 0.2967 - accuracy: 0.9223 - 21s/epoch - 256ms/step\n",
      "Epoch 71/80\n",
      "84/84 - 21s - loss: 0.2646 - accuracy: 0.9352 - 21s/epoch - 255ms/step\n",
      "Epoch 72/80\n",
      "84/84 - 22s - loss: 0.2682 - accuracy: 0.9325 - 22s/epoch - 256ms/step\n",
      "Epoch 73/80\n",
      "84/84 - 22s - loss: 0.2529 - accuracy: 0.9373 - 22s/epoch - 262ms/step\n",
      "Epoch 74/80\n",
      "84/84 - 22s - loss: 0.2410 - accuracy: 0.9411 - 22s/epoch - 260ms/step\n",
      "Epoch 75/80\n",
      "84/84 - 22s - loss: 0.2230 - accuracy: 0.9469 - 22s/epoch - 265ms/step\n",
      "Epoch 76/80\n",
      "84/84 - 23s - loss: 0.2085 - accuracy: 0.9513 - 23s/epoch - 268ms/step\n",
      "Epoch 77/80\n",
      "84/84 - 22s - loss: 0.2012 - accuracy: 0.9530 - 22s/epoch - 260ms/step\n",
      "Epoch 78/80\n",
      "84/84 - 22s - loss: 0.1999 - accuracy: 0.9526 - 22s/epoch - 256ms/step\n",
      "Epoch 79/80\n",
      "84/84 - 23s - loss: 0.2043 - accuracy: 0.9507 - 23s/epoch - 270ms/step\n",
      "Epoch 80/80\n",
      "84/84 - 22s - loss: 0.1938 - accuracy: 0.9533 - 22s/epoch - 259ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2cae5b212a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed\n",
    "\n",
    "hidden_units = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_units, input_shape=(None, train_X.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(hidden_units, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(vocab_size, activation = 'softmax')))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(train_X, train_y, epochs = 80, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980d60a",
   "metadata": {},
   "source": [
    "특정 문자를 주면 다음 문자를 계속해서 생성해내는 setence_generation 함수를 구현한다.\\\n",
    "인자로 학습한 모델, 그리고 모델로 다음 문자를 몇 번 생성할 것인지 횟수를 전달해주면, 해당 함수는 임의로 시작 문자를 정한 뒤에 정해진 횟수만큼의 다음 문자를 지속적으로 예측하여 문장을 생성해낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d6bd50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, length) :\n",
    "    # 문자에 대한 랜덤한 정수 생성\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    \n",
    "    # 랜덤한 정수로부터 맵핑되는 문자 생성\n",
    "    y_char = [index_to_char[ix[-1]]]\n",
    "    print(ix[-1],'번 문자',y_char[-1],'로 예측을 시작')\n",
    "    \n",
    "    # (1, length, 55) 크기의 X 생성. 즉, LSTM의 입력 시퀀스 생성\n",
    "    X = np.zeros ((1, length, vocab_size))\n",
    "    \n",
    "    for i in range(length) :\n",
    "        # X[0][i][예측한 문자의 인덱스] = 1, 즉, 예측 문자를 다음 입력 시퀀스에 추가\n",
    "        X[0][i][ix[-1]] = 1\n",
    "        print(index_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0],1)\n",
    "        y_char.append(index_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db37a5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 번 문자 v 로 예측을 시작\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "ver_ happen in a natural way again. i should like to here? as some down up and round alice, the queev\n"
     ]
    }
   ],
   "source": [
    "result = sentence_generation(model, 100)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f01b55",
   "metadata": {},
   "source": [
    "2. 문자 단위 RNN(Char RNN)으로 텍스트 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7696f179",
   "metadata": {},
   "source": [
    "이번에는 다 대 일 구조의 RNN을 문자 단위로 학습시키고, 텍스트 생성을 해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c751821",
   "metadata": {},
   "source": [
    "1) 데이터에 대한 이해와 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1239d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8eca8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 파일\n",
    "raw_text = '''\n",
    "I get on with life as a programmer,\n",
    "I like to contemplate beer.\n",
    "But when I start to daydream,\n",
    "My mind turns straight to wine.\n",
    "\n",
    "Do I love wine more than beer?\n",
    "\n",
    "I like to use words about beer.\n",
    "But when I stop my talking,\n",
    "My mind turns straight to wine.\n",
    "\n",
    "I hate bugs and errors.\n",
    "But I just think back to wine,\n",
    "And I'm happy once again.\n",
    "\n",
    "I like to hang out with programming and deep learning.\n",
    "But when left alone,\n",
    "My mind turns straight to wine.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dd75b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I get on with life as a programmer, I like to contemplate beer. But when I start to daydream, My mind turns straight to wine. Do I love wine more than beer? I like to use words about beer. But when I stop my talking, My mind turns straight to wine. I hate bugs and errors. But I just think back to wine, And I'm happy once again. I like to hang out with programming and deep learning. But when left alone, My mind turns straight to wine.\n"
     ]
    }
   ],
   "source": [
    "tokens = raw_text.split()\n",
    "raw_text = ' '.join(tokens)\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "813a1d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합 : [' ', \"'\", ',', '.', '?', 'A', 'B', 'D', 'I', 'M', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n",
      "문자 집합의 크기 : 33\n"
     ]
    }
   ],
   "source": [
    "# 중복을 제거한 문자 집합 생성\n",
    "char_vocab = sorted(list(set(raw_text)))\n",
    "vocab_size = len(char_vocab)\n",
    "print('문자 집합 :',char_vocab)\n",
    "print('문자 집합의 크기 : {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1f3d176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, \"'\": 1, ',': 2, '.': 3, '?': 4, 'A': 5, 'B': 6, 'D': 7, 'I': 8, 'M': 9, 'a': 10, 'b': 11, 'c': 12, 'd': 13, 'e': 14, 'f': 15, 'g': 16, 'h': 17, 'i': 18, 'j': 19, 'k': 20, 'l': 21, 'm': 22, 'n': 23, 'o': 24, 'p': 25, 'r': 26, 's': 27, 't': 28, 'u': 29, 'v': 30, 'w': 31, 'y': 32}\n"
     ]
    }
   ],
   "source": [
    "char_to_index = dict((char, index) for index, char in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
    "print(char_to_index)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHgAAAA6CAYAAAB79J21AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAHYcAAB2HAY/l8WUAAAULSURBVHhe7ZsxT+NKEMfH71OABEWkV3JNKhJBg8QHeIly0kuqp6tQumvhKCB0T3SI6umqGOlQ8gWQaEDxVTRQnuQCpORb+M2s1/baGJI1TgJz85NW9i5rr9m/d3aszDgBAgJb/tBHgSkiMHNEYOaIwMwRgZkjAjNHBGaOCMwctgJ7xw40Lya69vvyzgT24MRpgvukq8KbERPNnOUI/PMEHMeJy8nPqK0OBzCEzjq1n+B6BphcNMH57ELK2FLfTBuZ5Piex3Rllgm4nx1V4n7Z+87KkwvN3DFeIxyftg3zWee9jVgJ/PDwANvb2+rB6Eh1a2hyanfQfwyAfucIHvtQofbNfayPoAcN/bd9qKkLpkMTVr/vw5juR2X3GuqH+o8KmtxVGPw1VkX1wTL61IFVa6GQtTYMcIwiAg3bq3C9q5/T62G9O98tCQeama2tLfrlKS5Ut+axHzSgF4x0Nc0oQIEDFDhm7DYCaPUDFC/B6yVt6n7pa4jREQQNV19l9E/d59VnmQ31fDgXPU83vMg46Ldw3o7M0cK26dcWx2oF397e6rOQbH0m1nag2TqAelnm6clHo16Fypqu5zDx7wAucbXimFQi8+isd/DaO/DzVhBZGrPvC+Z85e+BWo07V9RvuoPY+FPZqxR3/vzMtJXAuGL1WUi2Phsr0P5BJmoEVTRXs0zKVFqV0My/Ris04VRIkKQMoJ33cpAZNvv9aOOT56BfhHCLeOFeS8RK4PPz81hUOlK9ODXYx4kbuwCd7y/vgyuVqj5LUCsyYq0CjcsBXKdekgn49/oUUfe49MHHcyqlEDmKXwHOXnsBloyVwBsbG3Bzc6PeaDpS3RqcGOU1a/xfQ30WMUybTCVgB/6LrsEV020b1yiTj5638ZJMLrrQudQVYvML9GlbQIeKSgJ+dxdxsmjVnlZCa/BOhY1BsRaLcmwSRy3tdCROCxjOT9KGhZwl08lSaAdG9yHnKuVkKciBQ4cmuo8qz52z+RE+Y/qZ8trKRWKymGNlooWPhwjMHBGYOSIwc0Rg5ojAzBGBmSMCM0cEZo4IzBwRmDkiMHNEYOaIwMz52AKrcJkwvFbIZzkC58Q1/14sLoNDTDR3VFzHAqFQGho2LipkR4fTpOKDc2KGU+E+jaDv5cQ1Z0KCzHAYCv2J46wp7Cfqlwr/sYDGyoQcTcUcV5W3xWVPw2oFl5HZUPsWqIj+KIw1+DZj/gLtt+sdqHp4DV0XnAGcUlyzgeozgGaUNaFDc80gPwXFSF/txH16WO8WidEukuHwhgyOIlgJvLe3Fwe705Hqi8L7jmIejWB/UzdQfPVpH6cpgfqAe2bEJtfgi9uAg6usG9aDUfxihX2GvwoG1CrBAjiDrhL62cu0ZKwELiWzoRBhnHNeVkBC2Idyf1S8si6rFGJ776cdurxA+WyfCOWpJ/crK8NhUVgJXE5mQ3GqlekRyLjnqolOlbfELn/wDAcrgcvNbDDBFdXSpzE++HHw+gpUPsFzU6vykiLCPoVNbVHeeYaDlcClZDZE6FSSEC3gaWL+vGPKFU6o7aJjdvivYfrwW7Jm9sA+/+CefFhP74OZTIpSoVVbOMMhk8ExL1CsJWBkIsSfGeGnkmrD0vNyPpNSnxj4eZGX/pn9DDE+gVKfSbqNyGubN2rM6P/QbfNAMhuYY2WihY+HCMwcEZg5IjBzRGDmiMDMEYGZIwIzRwRmjgjMHBGYNQD/A0p7g3RYjM5yAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "id": "6c93f1a9",
   "metadata": {},
   "source": [
    "이번 문자 집합의 경우 알파벳의 대,소문자를 구분하고 구두점과 공백을 포함하였다.\\\n",
    "이제 훈련에 사용할 문장 샘플들을 만들어보자, 여기서는 RNN을 이용하여 생성한 텍스트 챕터와 유사하게 데이터를 구성한다.\\\n",
    "다만, 단위가 문자 단위라는 점이 다르다.\\\n",
    "ex) student라는 단어가 있고, 입력 시퀀스의 길이를 5라고 한다면 입력 시퀀스와 예측해야하는 문자는 다음과 같이 구성된다.\\\n",
    "![image.png](attachment:image.png)\\\n",
    "즉, RNN의 시점(timesteps)은 5번이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2efbd4",
   "metadata": {},
   "source": [
    "여기서는 입력 시퀀스의 길이가 10이 되도록 데이터를 구성하였다. 예측 대상인 문자도 필요하므로 길이가 11이 되도록 데이터를 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64d00450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 훈련 샘플의 수 : 426\n"
     ]
    }
   ],
   "source": [
    "length = 11\n",
    "sequences = []\n",
    "for i in range(length, len(raw_text)):\n",
    "    seq = raw_text[i-length:i] # 길이 11의 문자열을 지속적으로 만든다.(11~437)\n",
    "    sequences.append(seq)\n",
    "print('총 훈련 샘플의 수 : %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a0ff0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I get on wi',\n",
       " ' get on wit',\n",
       " 'get on with',\n",
       " 'et on with ',\n",
       " 't on with l',\n",
       " ' on with li',\n",
       " 'on with lif',\n",
       " 'n with life',\n",
       " ' with life ',\n",
       " 'with life a']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f3e17",
   "metadata": {},
   "source": [
    "첫번째 문장이었던 'I get on with life as a programmer'가 여러개의 샘플로 분리된 것을 확인할 수 있다.\\\n",
    "이제 앞서 만든 char_to_index를 사용하여 전체 데이터에 대해서 정수 인코딩을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6dd540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences = []\n",
    "for sequence in sequences: # 전체 데이터에서 문장 샘플을 1개씩 꺼낸다.\n",
    "    encoded_sequence = [char_to_index[char] for char in sequence] # 문장 샘플에서 각 문자에 대해서 정수 인코딩을 수행\n",
    "    encoded_sequences.append(encoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ad2cc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 0, 16, 14, 28, 0, 24, 23, 0, 31, 18],\n",
       " [0, 16, 14, 28, 0, 24, 23, 0, 31, 18, 28],\n",
       " [16, 14, 28, 0, 24, 23, 0, 31, 18, 28, 17],\n",
       " [14, 28, 0, 24, 23, 0, 31, 18, 28, 17, 0],\n",
       " [28, 0, 24, 23, 0, 31, 18, 28, 17, 0, 21]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d617b",
   "metadata": {},
   "source": [
    "예측 대상인 문자를 분리시켜주는 작업을 진행해보자. 모든 샘플 문장에 대해서 마지막 문자를 분리하여 마지막 문자가 분리된 샘플은\\\n",
    "X_data에 저장하고, 마지막 문자는 y_data에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e57142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences = np.array(encoded_sequences)\n",
    "\n",
    "# 맨 마지막 위치의 문자를 분리\n",
    "X_data = encoded_sequences[:,:-1]\n",
    "# 맨 마지막 위치의 문자를 저장\n",
    "y_data = encoded_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29326a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  0 16 14 28  0 24 23  0 31]\n",
      " [ 0 16 14 28  0 24 23  0 31 18]\n",
      " [16 14 28  0 24 23  0 31 18 28]\n",
      " [14 28  0 24 23  0 31 18 28 17]\n",
      " [28  0 24 23  0 31 18 28 17  0]]\n",
      "[18 28 17  0 21]\n"
     ]
    }
   ],
   "source": [
    "print(X_data[:5])\n",
    "print(y_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "321952a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원-핫 인코딩\n",
    "X_data_one_hot = [to_categorical(encoded, num_classes = vocab_size) for encoded in X_data]\n",
    "X_data_one_hot = np.array(X_data_one_hot)\n",
    "y_data_one_hot = to_categorical(y_data, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "572afd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 10, 33)\n"
     ]
    }
   ],
   "source": [
    "print(X_data_one_hot.shape)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "40899271",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\\\n",
    "샘플의 수가 426개, 입력 시퀀스의 길이가 10, 각 벡터의 차원이 33임을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856fe31",
   "metadata": {},
   "source": [
    "2) 모델 설계하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542405cc",
   "metadata": {},
   "source": [
    "하이퍼파라미터인 은닉 상태의 크기는 64, 모델은 다 대 일 구조의 LSTM을 사용한다. \\\n",
    "전결합층을 출력층으로 문자 집합 크기만큼의 뉴런을 배치하여 모델을 설계한다. 해당 모델은 마지막 시점에서 모든 사용 가능한 문자 중 하나의 문자를 예측하는 다중 클래스 분류 문제를 수행하는 모델이다.\\\n",
    "출력층에 소프트맥스 회귀를 사용해야 하므로 활성화 함수로는 소프트맥스 함수, 손실 함수로 크로스 엔트로피 함수를 사용하여 200에포크를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51058383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "14/14 - 2s - loss: 3.4708 - accuracy: 0.1080 - 2s/epoch - 157ms/step\n",
      "Epoch 2/300\n",
      "14/14 - 0s - loss: 3.3624 - accuracy: 0.1972 - 82ms/epoch - 6ms/step\n",
      "Epoch 3/300\n",
      "14/14 - 0s - loss: 3.1025 - accuracy: 0.1972 - 76ms/epoch - 5ms/step\n",
      "Epoch 4/300\n",
      "14/14 - 0s - loss: 3.0115 - accuracy: 0.1972 - 77ms/epoch - 6ms/step\n",
      "Epoch 5/300\n",
      "14/14 - 0s - loss: 2.9563 - accuracy: 0.1972 - 87ms/epoch - 6ms/step\n",
      "Epoch 6/300\n",
      "14/14 - 0s - loss: 2.9352 - accuracy: 0.1972 - 82ms/epoch - 6ms/step\n",
      "Epoch 7/300\n",
      "14/14 - 0s - loss: 2.9118 - accuracy: 0.1972 - 77ms/epoch - 5ms/step\n",
      "Epoch 8/300\n",
      "14/14 - 0s - loss: 2.8892 - accuracy: 0.1972 - 76ms/epoch - 5ms/step\n",
      "Epoch 9/300\n",
      "14/14 - 0s - loss: 2.8602 - accuracy: 0.1972 - 82ms/epoch - 6ms/step\n",
      "Epoch 10/300\n",
      "14/14 - 0s - loss: 2.8418 - accuracy: 0.1972 - 83ms/epoch - 6ms/step\n",
      "Epoch 11/300\n",
      "14/14 - 0s - loss: 2.8145 - accuracy: 0.2113 - 79ms/epoch - 6ms/step\n",
      "Epoch 12/300\n",
      "14/14 - 0s - loss: 2.7710 - accuracy: 0.2019 - 80ms/epoch - 6ms/step\n",
      "Epoch 13/300\n",
      "14/14 - 0s - loss: 2.7326 - accuracy: 0.2183 - 78ms/epoch - 6ms/step\n",
      "Epoch 14/300\n",
      "14/14 - 0s - loss: 2.6918 - accuracy: 0.2113 - 78ms/epoch - 6ms/step\n",
      "Epoch 15/300\n",
      "14/14 - 0s - loss: 2.6442 - accuracy: 0.2347 - 78ms/epoch - 6ms/step\n",
      "Epoch 16/300\n",
      "14/14 - 0s - loss: 2.5790 - accuracy: 0.2535 - 80ms/epoch - 6ms/step\n",
      "Epoch 17/300\n",
      "14/14 - 0s - loss: 2.5428 - accuracy: 0.2676 - 79ms/epoch - 6ms/step\n",
      "Epoch 18/300\n",
      "14/14 - 0s - loss: 2.4889 - accuracy: 0.2840 - 79ms/epoch - 6ms/step\n",
      "Epoch 19/300\n",
      "14/14 - 0s - loss: 2.4433 - accuracy: 0.3005 - 84ms/epoch - 6ms/step\n",
      "Epoch 20/300\n",
      "14/14 - 0s - loss: 2.4174 - accuracy: 0.3333 - 79ms/epoch - 6ms/step\n",
      "Epoch 21/300\n",
      "14/14 - 0s - loss: 2.3742 - accuracy: 0.3028 - 81ms/epoch - 6ms/step\n",
      "Epoch 22/300\n",
      "14/14 - 0s - loss: 2.3128 - accuracy: 0.3451 - 82ms/epoch - 6ms/step\n",
      "Epoch 23/300\n",
      "14/14 - 0s - loss: 2.2603 - accuracy: 0.3545 - 79ms/epoch - 6ms/step\n",
      "Epoch 24/300\n",
      "14/14 - 0s - loss: 2.2069 - accuracy: 0.3826 - 78ms/epoch - 6ms/step\n",
      "Epoch 25/300\n",
      "14/14 - 0s - loss: 2.1788 - accuracy: 0.3850 - 79ms/epoch - 6ms/step\n",
      "Epoch 26/300\n",
      "14/14 - 0s - loss: 2.1393 - accuracy: 0.4061 - 81ms/epoch - 6ms/step\n",
      "Epoch 27/300\n",
      "14/14 - 0s - loss: 2.1093 - accuracy: 0.4038 - 83ms/epoch - 6ms/step\n",
      "Epoch 28/300\n",
      "14/14 - 0s - loss: 2.0533 - accuracy: 0.4366 - 80ms/epoch - 6ms/step\n",
      "Epoch 29/300\n",
      "14/14 - 0s - loss: 2.0120 - accuracy: 0.4366 - 82ms/epoch - 6ms/step\n",
      "Epoch 30/300\n",
      "14/14 - 0s - loss: 1.9715 - accuracy: 0.4554 - 75ms/epoch - 5ms/step\n",
      "Epoch 31/300\n",
      "14/14 - 0s - loss: 1.9373 - accuracy: 0.4484 - 80ms/epoch - 6ms/step\n",
      "Epoch 32/300\n",
      "14/14 - 0s - loss: 1.8963 - accuracy: 0.4836 - 79ms/epoch - 6ms/step\n",
      "Epoch 33/300\n",
      "14/14 - 0s - loss: 1.8591 - accuracy: 0.4930 - 78ms/epoch - 6ms/step\n",
      "Epoch 34/300\n",
      "14/14 - 0s - loss: 1.7880 - accuracy: 0.5023 - 89ms/epoch - 6ms/step\n",
      "Epoch 35/300\n",
      "14/14 - 0s - loss: 1.7746 - accuracy: 0.5164 - 82ms/epoch - 6ms/step\n",
      "Epoch 36/300\n",
      "14/14 - 0s - loss: 1.7335 - accuracy: 0.5493 - 92ms/epoch - 7ms/step\n",
      "Epoch 37/300\n",
      "14/14 - 0s - loss: 1.6915 - accuracy: 0.5493 - 81ms/epoch - 6ms/step\n",
      "Epoch 38/300\n",
      "14/14 - 0s - loss: 1.6510 - accuracy: 0.5681 - 82ms/epoch - 6ms/step\n",
      "Epoch 39/300\n",
      "14/14 - 0s - loss: 1.6240 - accuracy: 0.5892 - 89ms/epoch - 6ms/step\n",
      "Epoch 40/300\n",
      "14/14 - 0s - loss: 1.6070 - accuracy: 0.5986 - 102ms/epoch - 7ms/step\n",
      "Epoch 41/300\n",
      "14/14 - 0s - loss: 1.5443 - accuracy: 0.6150 - 89ms/epoch - 6ms/step\n",
      "Epoch 42/300\n",
      "14/14 - 0s - loss: 1.5275 - accuracy: 0.6174 - 91ms/epoch - 7ms/step\n",
      "Epoch 43/300\n",
      "14/14 - 0s - loss: 1.4504 - accuracy: 0.6479 - 77ms/epoch - 5ms/step\n",
      "Epoch 44/300\n",
      "14/14 - 0s - loss: 1.4279 - accuracy: 0.6244 - 87ms/epoch - 6ms/step\n",
      "Epoch 45/300\n",
      "14/14 - 0s - loss: 1.3722 - accuracy: 0.6714 - 80ms/epoch - 6ms/step\n",
      "Epoch 46/300\n",
      "14/14 - 0s - loss: 1.3666 - accuracy: 0.6667 - 88ms/epoch - 6ms/step\n",
      "Epoch 47/300\n",
      "14/14 - 0s - loss: 1.3228 - accuracy: 0.6854 - 89ms/epoch - 6ms/step\n",
      "Epoch 48/300\n",
      "14/14 - 0s - loss: 1.2883 - accuracy: 0.6878 - 93ms/epoch - 7ms/step\n",
      "Epoch 49/300\n",
      "14/14 - 0s - loss: 1.2568 - accuracy: 0.7042 - 79ms/epoch - 6ms/step\n",
      "Epoch 50/300\n",
      "14/14 - 0s - loss: 1.2203 - accuracy: 0.7136 - 76ms/epoch - 5ms/step\n",
      "Epoch 51/300\n",
      "14/14 - 0s - loss: 1.1882 - accuracy: 0.7113 - 83ms/epoch - 6ms/step\n",
      "Epoch 52/300\n",
      "14/14 - 0s - loss: 1.1667 - accuracy: 0.7300 - 91ms/epoch - 7ms/step\n",
      "Epoch 53/300\n",
      "14/14 - 0s - loss: 1.1182 - accuracy: 0.7582 - 82ms/epoch - 6ms/step\n",
      "Epoch 54/300\n",
      "14/14 - 0s - loss: 1.0885 - accuracy: 0.7770 - 86ms/epoch - 6ms/step\n",
      "Epoch 55/300\n",
      "14/14 - 0s - loss: 1.0677 - accuracy: 0.7441 - 87ms/epoch - 6ms/step\n",
      "Epoch 56/300\n",
      "14/14 - 0s - loss: 1.0736 - accuracy: 0.7676 - 77ms/epoch - 6ms/step\n",
      "Epoch 57/300\n",
      "14/14 - 0s - loss: 1.0097 - accuracy: 0.7723 - 82ms/epoch - 6ms/step\n",
      "Epoch 58/300\n",
      "14/14 - 0s - loss: 0.9911 - accuracy: 0.7911 - 82ms/epoch - 6ms/step\n",
      "Epoch 59/300\n",
      "14/14 - 0s - loss: 0.9605 - accuracy: 0.8028 - 80ms/epoch - 6ms/step\n",
      "Epoch 60/300\n",
      "14/14 - 0s - loss: 0.9260 - accuracy: 0.8146 - 78ms/epoch - 6ms/step\n",
      "Epoch 61/300\n",
      "14/14 - 0s - loss: 0.9037 - accuracy: 0.8216 - 81ms/epoch - 6ms/step\n",
      "Epoch 62/300\n",
      "14/14 - 0s - loss: 0.8792 - accuracy: 0.8286 - 78ms/epoch - 6ms/step\n",
      "Epoch 63/300\n",
      "14/14 - 0s - loss: 0.8615 - accuracy: 0.8357 - 81ms/epoch - 6ms/step\n",
      "Epoch 64/300\n",
      "14/14 - 0s - loss: 0.8376 - accuracy: 0.8333 - 80ms/epoch - 6ms/step\n",
      "Epoch 65/300\n",
      "14/14 - 0s - loss: 0.8061 - accuracy: 0.8427 - 83ms/epoch - 6ms/step\n",
      "Epoch 66/300\n",
      "14/14 - 0s - loss: 0.7862 - accuracy: 0.8545 - 81ms/epoch - 6ms/step\n",
      "Epoch 67/300\n",
      "14/14 - 0s - loss: 0.7671 - accuracy: 0.8521 - 79ms/epoch - 6ms/step\n",
      "Epoch 68/300\n",
      "14/14 - 0s - loss: 0.7476 - accuracy: 0.8521 - 79ms/epoch - 6ms/step\n",
      "Epoch 69/300\n",
      "14/14 - 0s - loss: 0.7324 - accuracy: 0.8638 - 85ms/epoch - 6ms/step\n",
      "Epoch 70/300\n",
      "14/14 - 0s - loss: 0.7088 - accuracy: 0.8568 - 84ms/epoch - 6ms/step\n",
      "Epoch 71/300\n",
      "14/14 - 0s - loss: 0.6823 - accuracy: 0.8803 - 81ms/epoch - 6ms/step\n",
      "Epoch 72/300\n",
      "14/14 - 0s - loss: 0.6643 - accuracy: 0.8756 - 86ms/epoch - 6ms/step\n",
      "Epoch 73/300\n",
      "14/14 - 0s - loss: 0.6442 - accuracy: 0.8779 - 86ms/epoch - 6ms/step\n",
      "Epoch 74/300\n",
      "14/14 - 0s - loss: 0.6284 - accuracy: 0.8920 - 83ms/epoch - 6ms/step\n",
      "Epoch 75/300\n",
      "14/14 - 0s - loss: 0.6157 - accuracy: 0.9014 - 83ms/epoch - 6ms/step\n",
      "Epoch 76/300\n",
      "14/14 - 0s - loss: 0.5968 - accuracy: 0.9061 - 76ms/epoch - 5ms/step\n",
      "Epoch 77/300\n",
      "14/14 - 0s - loss: 0.5765 - accuracy: 0.9061 - 87ms/epoch - 6ms/step\n",
      "Epoch 78/300\n",
      "14/14 - 0s - loss: 0.5749 - accuracy: 0.8944 - 85ms/epoch - 6ms/step\n",
      "Epoch 79/300\n",
      "14/14 - 0s - loss: 0.5425 - accuracy: 0.9155 - 89ms/epoch - 6ms/step\n",
      "Epoch 80/300\n",
      "14/14 - 0s - loss: 0.5265 - accuracy: 0.9155 - 85ms/epoch - 6ms/step\n",
      "Epoch 81/300\n",
      "14/14 - 0s - loss: 0.5115 - accuracy: 0.9131 - 85ms/epoch - 6ms/step\n",
      "Epoch 82/300\n",
      "14/14 - 0s - loss: 0.4969 - accuracy: 0.9225 - 80ms/epoch - 6ms/step\n",
      "Epoch 83/300\n",
      "14/14 - 0s - loss: 0.4788 - accuracy: 0.9296 - 84ms/epoch - 6ms/step\n",
      "Epoch 84/300\n",
      "14/14 - 0s - loss: 0.4679 - accuracy: 0.9366 - 83ms/epoch - 6ms/step\n",
      "Epoch 85/300\n",
      "14/14 - 0s - loss: 0.4590 - accuracy: 0.9272 - 83ms/epoch - 6ms/step\n",
      "Epoch 86/300\n",
      "14/14 - 0s - loss: 0.4525 - accuracy: 0.9343 - 85ms/epoch - 6ms/step\n",
      "Epoch 87/300\n",
      "14/14 - 0s - loss: 0.4278 - accuracy: 0.9366 - 83ms/epoch - 6ms/step\n",
      "Epoch 88/300\n",
      "14/14 - 0s - loss: 0.4245 - accuracy: 0.9390 - 85ms/epoch - 6ms/step\n",
      "Epoch 89/300\n",
      "14/14 - 0s - loss: 0.4254 - accuracy: 0.9484 - 88ms/epoch - 6ms/step\n",
      "Epoch 90/300\n",
      "14/14 - 0s - loss: 0.4055 - accuracy: 0.9390 - 100ms/epoch - 7ms/step\n",
      "Epoch 91/300\n",
      "14/14 - 0s - loss: 0.4042 - accuracy: 0.9413 - 83ms/epoch - 6ms/step\n",
      "Epoch 92/300\n",
      "14/14 - 0s - loss: 0.3890 - accuracy: 0.9390 - 81ms/epoch - 6ms/step\n",
      "Epoch 93/300\n",
      "14/14 - 0s - loss: 0.3723 - accuracy: 0.9554 - 81ms/epoch - 6ms/step\n",
      "Epoch 94/300\n",
      "14/14 - 0s - loss: 0.3564 - accuracy: 0.9601 - 76ms/epoch - 5ms/step\n",
      "Epoch 95/300\n",
      "14/14 - 0s - loss: 0.3478 - accuracy: 0.9531 - 79ms/epoch - 6ms/step\n",
      "Epoch 96/300\n",
      "14/14 - 0s - loss: 0.3421 - accuracy: 0.9624 - 79ms/epoch - 6ms/step\n",
      "Epoch 97/300\n",
      "14/14 - 0s - loss: 0.3307 - accuracy: 0.9601 - 89ms/epoch - 6ms/step\n",
      "Epoch 98/300\n",
      "14/14 - 0s - loss: 0.3219 - accuracy: 0.9671 - 95ms/epoch - 7ms/step\n",
      "Epoch 99/300\n",
      "14/14 - 0s - loss: 0.3169 - accuracy: 0.9648 - 104ms/epoch - 7ms/step\n",
      "Epoch 100/300\n",
      "14/14 - 0s - loss: 0.3067 - accuracy: 0.9718 - 139ms/epoch - 10ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/300\n",
      "14/14 - 0s - loss: 0.3047 - accuracy: 0.9718 - 101ms/epoch - 7ms/step\n",
      "Epoch 102/300\n",
      "14/14 - 0s - loss: 0.2954 - accuracy: 0.9765 - 92ms/epoch - 7ms/step\n",
      "Epoch 103/300\n",
      "14/14 - 0s - loss: 0.2786 - accuracy: 0.9742 - 86ms/epoch - 6ms/step\n",
      "Epoch 104/300\n",
      "14/14 - 0s - loss: 0.2698 - accuracy: 0.9671 - 79ms/epoch - 6ms/step\n",
      "Epoch 105/300\n",
      "14/14 - 0s - loss: 0.2577 - accuracy: 0.9765 - 76ms/epoch - 5ms/step\n",
      "Epoch 106/300\n",
      "14/14 - 0s - loss: 0.2663 - accuracy: 0.9742 - 83ms/epoch - 6ms/step\n",
      "Epoch 107/300\n",
      "14/14 - 0s - loss: 0.2505 - accuracy: 0.9789 - 78ms/epoch - 6ms/step\n",
      "Epoch 108/300\n",
      "14/14 - 0s - loss: 0.2426 - accuracy: 0.9765 - 81ms/epoch - 6ms/step\n",
      "Epoch 109/300\n",
      "14/14 - 0s - loss: 0.2397 - accuracy: 0.9695 - 78ms/epoch - 6ms/step\n",
      "Epoch 110/300\n",
      "14/14 - 0s - loss: 0.2301 - accuracy: 0.9789 - 83ms/epoch - 6ms/step\n",
      "Epoch 111/300\n",
      "14/14 - 0s - loss: 0.2263 - accuracy: 0.9836 - 84ms/epoch - 6ms/step\n",
      "Epoch 112/300\n",
      "14/14 - 0s - loss: 0.2199 - accuracy: 0.9765 - 86ms/epoch - 6ms/step\n",
      "Epoch 113/300\n",
      "14/14 - 0s - loss: 0.2140 - accuracy: 0.9765 - 92ms/epoch - 7ms/step\n",
      "Epoch 114/300\n",
      "14/14 - 0s - loss: 0.2061 - accuracy: 0.9812 - 83ms/epoch - 6ms/step\n",
      "Epoch 115/300\n",
      "14/14 - 0s - loss: 0.2135 - accuracy: 0.9742 - 78ms/epoch - 6ms/step\n",
      "Epoch 116/300\n",
      "14/14 - 0s - loss: 0.1985 - accuracy: 0.9789 - 77ms/epoch - 5ms/step\n",
      "Epoch 117/300\n",
      "14/14 - 0s - loss: 0.1949 - accuracy: 0.9836 - 84ms/epoch - 6ms/step\n",
      "Epoch 118/300\n",
      "14/14 - 0s - loss: 0.1899 - accuracy: 0.9836 - 89ms/epoch - 6ms/step\n",
      "Epoch 119/300\n",
      "14/14 - 0s - loss: 0.1894 - accuracy: 0.9765 - 73ms/epoch - 5ms/step\n",
      "Epoch 120/300\n",
      "14/14 - 0s - loss: 0.1826 - accuracy: 0.9789 - 76ms/epoch - 5ms/step\n",
      "Epoch 121/300\n",
      "14/14 - 0s - loss: 0.2051 - accuracy: 0.9671 - 82ms/epoch - 6ms/step\n",
      "Epoch 122/300\n",
      "14/14 - 0s - loss: 0.1814 - accuracy: 0.9812 - 92ms/epoch - 7ms/step\n",
      "Epoch 123/300\n",
      "14/14 - 0s - loss: 0.1769 - accuracy: 0.9859 - 80ms/epoch - 6ms/step\n",
      "Epoch 124/300\n",
      "14/14 - 0s - loss: 0.1821 - accuracy: 0.9765 - 95ms/epoch - 7ms/step\n",
      "Epoch 125/300\n",
      "14/14 - 0s - loss: 0.1694 - accuracy: 0.9812 - 83ms/epoch - 6ms/step\n",
      "Epoch 126/300\n",
      "14/14 - 0s - loss: 0.1737 - accuracy: 0.9812 - 73ms/epoch - 5ms/step\n",
      "Epoch 127/300\n",
      "14/14 - 0s - loss: 0.1762 - accuracy: 0.9742 - 80ms/epoch - 6ms/step\n",
      "Epoch 128/300\n",
      "14/14 - 0s - loss: 0.1683 - accuracy: 0.9765 - 89ms/epoch - 6ms/step\n",
      "Epoch 129/300\n",
      "14/14 - 0s - loss: 0.1555 - accuracy: 0.9765 - 115ms/epoch - 8ms/step\n",
      "Epoch 130/300\n",
      "14/14 - 0s - loss: 0.1498 - accuracy: 0.9789 - 83ms/epoch - 6ms/step\n",
      "Epoch 131/300\n",
      "14/14 - 0s - loss: 0.1469 - accuracy: 0.9859 - 87ms/epoch - 6ms/step\n",
      "Epoch 132/300\n",
      "14/14 - 0s - loss: 0.1442 - accuracy: 0.9812 - 87ms/epoch - 6ms/step\n",
      "Epoch 133/300\n",
      "14/14 - 0s - loss: 0.1463 - accuracy: 0.9836 - 77ms/epoch - 5ms/step\n",
      "Epoch 134/300\n",
      "14/14 - 0s - loss: 0.1423 - accuracy: 0.9812 - 78ms/epoch - 6ms/step\n",
      "Epoch 135/300\n",
      "14/14 - 0s - loss: 0.1412 - accuracy: 0.9836 - 75ms/epoch - 5ms/step\n",
      "Epoch 136/300\n",
      "14/14 - 0s - loss: 0.1417 - accuracy: 0.9812 - 77ms/epoch - 5ms/step\n",
      "Epoch 137/300\n",
      "14/14 - 0s - loss: 0.1364 - accuracy: 0.9765 - 80ms/epoch - 6ms/step\n",
      "Epoch 138/300\n",
      "14/14 - 0s - loss: 0.1295 - accuracy: 0.9812 - 79ms/epoch - 6ms/step\n",
      "Epoch 139/300\n",
      "14/14 - 0s - loss: 0.1234 - accuracy: 0.9859 - 78ms/epoch - 6ms/step\n",
      "Epoch 140/300\n",
      "14/14 - 0s - loss: 0.1223 - accuracy: 0.9836 - 84ms/epoch - 6ms/step\n",
      "Epoch 141/300\n",
      "14/14 - 0s - loss: 0.1201 - accuracy: 0.9789 - 78ms/epoch - 6ms/step\n",
      "Epoch 142/300\n",
      "14/14 - 0s - loss: 0.1214 - accuracy: 0.9836 - 78ms/epoch - 6ms/step\n",
      "Epoch 143/300\n",
      "14/14 - 0s - loss: 0.1172 - accuracy: 0.9859 - 75ms/epoch - 5ms/step\n",
      "Epoch 144/300\n",
      "14/14 - 0s - loss: 0.1137 - accuracy: 0.9836 - 77ms/epoch - 5ms/step\n",
      "Epoch 145/300\n",
      "14/14 - 0s - loss: 0.1136 - accuracy: 0.9859 - 74ms/epoch - 5ms/step\n",
      "Epoch 146/300\n",
      "14/14 - 0s - loss: 0.1110 - accuracy: 0.9836 - 75ms/epoch - 5ms/step\n",
      "Epoch 147/300\n",
      "14/14 - 0s - loss: 0.1108 - accuracy: 0.9812 - 77ms/epoch - 5ms/step\n",
      "Epoch 148/300\n",
      "14/14 - 0s - loss: 0.1100 - accuracy: 0.9859 - 78ms/epoch - 6ms/step\n",
      "Epoch 149/300\n",
      "14/14 - 0s - loss: 0.1060 - accuracy: 0.9836 - 90ms/epoch - 6ms/step\n",
      "Epoch 150/300\n",
      "14/14 - 0s - loss: 0.1044 - accuracy: 0.9859 - 77ms/epoch - 5ms/step\n",
      "Epoch 151/300\n",
      "14/14 - 0s - loss: 0.1007 - accuracy: 0.9859 - 78ms/epoch - 6ms/step\n",
      "Epoch 152/300\n",
      "14/14 - 0s - loss: 0.1010 - accuracy: 0.9859 - 79ms/epoch - 6ms/step\n",
      "Epoch 153/300\n",
      "14/14 - 0s - loss: 0.1033 - accuracy: 0.9836 - 76ms/epoch - 5ms/step\n",
      "Epoch 154/300\n",
      "14/14 - 0s - loss: 0.1038 - accuracy: 0.9836 - 78ms/epoch - 6ms/step\n",
      "Epoch 155/300\n",
      "14/14 - 0s - loss: 0.0994 - accuracy: 0.9836 - 80ms/epoch - 6ms/step\n",
      "Epoch 156/300\n",
      "14/14 - 0s - loss: 0.0949 - accuracy: 0.9859 - 77ms/epoch - 5ms/step\n",
      "Epoch 157/300\n",
      "14/14 - 0s - loss: 0.0968 - accuracy: 0.9812 - 78ms/epoch - 6ms/step\n",
      "Epoch 158/300\n",
      "14/14 - 0s - loss: 0.1032 - accuracy: 0.9789 - 81ms/epoch - 6ms/step\n",
      "Epoch 159/300\n",
      "14/14 - 0s - loss: 0.0963 - accuracy: 0.9812 - 77ms/epoch - 5ms/step\n",
      "Epoch 160/300\n",
      "14/14 - 0s - loss: 0.0903 - accuracy: 0.9836 - 77ms/epoch - 5ms/step\n",
      "Epoch 161/300\n",
      "14/14 - 0s - loss: 0.0890 - accuracy: 0.9859 - 81ms/epoch - 6ms/step\n",
      "Epoch 162/300\n",
      "14/14 - 0s - loss: 0.0867 - accuracy: 0.9836 - 75ms/epoch - 5ms/step\n",
      "Epoch 163/300\n",
      "14/14 - 0s - loss: 0.0900 - accuracy: 0.9789 - 76ms/epoch - 5ms/step\n",
      "Epoch 164/300\n",
      "14/14 - 0s - loss: 0.0855 - accuracy: 0.9812 - 78ms/epoch - 6ms/step\n",
      "Epoch 165/300\n",
      "14/14 - 0s - loss: 0.0838 - accuracy: 0.9859 - 78ms/epoch - 6ms/step\n",
      "Epoch 166/300\n",
      "14/14 - 0s - loss: 0.0862 - accuracy: 0.9883 - 77ms/epoch - 6ms/step\n",
      "Epoch 167/300\n",
      "14/14 - 0s - loss: 0.0910 - accuracy: 0.9836 - 76ms/epoch - 5ms/step\n",
      "Epoch 168/300\n",
      "14/14 - 0s - loss: 0.0806 - accuracy: 0.9836 - 76ms/epoch - 5ms/step\n",
      "Epoch 169/300\n",
      "14/14 - 0s - loss: 0.0796 - accuracy: 0.9836 - 78ms/epoch - 6ms/step\n",
      "Epoch 170/300\n",
      "14/14 - 0s - loss: 0.0769 - accuracy: 0.9859 - 77ms/epoch - 6ms/step\n",
      "Epoch 171/300\n",
      "14/14 - 0s - loss: 0.0761 - accuracy: 0.9836 - 78ms/epoch - 6ms/step\n",
      "Epoch 172/300\n",
      "14/14 - 0s - loss: 0.0767 - accuracy: 0.9836 - 81ms/epoch - 6ms/step\n",
      "Epoch 173/300\n",
      "14/14 - 0s - loss: 0.0787 - accuracy: 0.9859 - 77ms/epoch - 6ms/step\n",
      "Epoch 174/300\n",
      "14/14 - 0s - loss: 0.0773 - accuracy: 0.9836 - 86ms/epoch - 6ms/step\n",
      "Epoch 175/300\n",
      "14/14 - 0s - loss: 0.0748 - accuracy: 0.9883 - 85ms/epoch - 6ms/step\n",
      "Epoch 176/300\n",
      "14/14 - 0s - loss: 0.0727 - accuracy: 0.9836 - 77ms/epoch - 5ms/step\n",
      "Epoch 177/300\n",
      "14/14 - 0s - loss: 0.0711 - accuracy: 0.9859 - 76ms/epoch - 5ms/step\n",
      "Epoch 178/300\n",
      "14/14 - 0s - loss: 0.0763 - accuracy: 0.9836 - 77ms/epoch - 5ms/step\n",
      "Epoch 179/300\n",
      "14/14 - 0s - loss: 0.0754 - accuracy: 0.9859 - 77ms/epoch - 5ms/step\n",
      "Epoch 180/300\n",
      "14/14 - 0s - loss: 0.0825 - accuracy: 0.9836 - 79ms/epoch - 6ms/step\n",
      "Epoch 181/300\n",
      "14/14 - 0s - loss: 0.0741 - accuracy: 0.9859 - 76ms/epoch - 5ms/step\n",
      "Epoch 182/300\n",
      "14/14 - 0s - loss: 0.0717 - accuracy: 0.9859 - 79ms/epoch - 6ms/step\n",
      "Epoch 183/300\n",
      "14/14 - 0s - loss: 0.0735 - accuracy: 0.9836 - 79ms/epoch - 6ms/step\n",
      "Epoch 184/300\n",
      "14/14 - 0s - loss: 0.0748 - accuracy: 0.9859 - 77ms/epoch - 5ms/step\n",
      "Epoch 185/300\n",
      "14/14 - 0s - loss: 0.0687 - accuracy: 0.9859 - 76ms/epoch - 5ms/step\n",
      "Epoch 186/300\n",
      "14/14 - 0s - loss: 0.0668 - accuracy: 0.9883 - 77ms/epoch - 5ms/step\n",
      "Epoch 187/300\n",
      "14/14 - 0s - loss: 0.0646 - accuracy: 0.9836 - 82ms/epoch - 6ms/step\n",
      "Epoch 188/300\n",
      "14/14 - 0s - loss: 0.0646 - accuracy: 0.9836 - 80ms/epoch - 6ms/step\n",
      "Epoch 189/300\n",
      "14/14 - 0s - loss: 0.0626 - accuracy: 0.9883 - 81ms/epoch - 6ms/step\n",
      "Epoch 190/300\n",
      "14/14 - 0s - loss: 0.0640 - accuracy: 0.9859 - 79ms/epoch - 6ms/step\n",
      "Epoch 191/300\n",
      "14/14 - 0s - loss: 0.0668 - accuracy: 0.9812 - 74ms/epoch - 5ms/step\n",
      "Epoch 192/300\n",
      "14/14 - 0s - loss: 0.0631 - accuracy: 0.9836 - 85ms/epoch - 6ms/step\n",
      "Epoch 193/300\n",
      "14/14 - 0s - loss: 0.0633 - accuracy: 0.9883 - 87ms/epoch - 6ms/step\n",
      "Epoch 194/300\n",
      "14/14 - 0s - loss: 0.0638 - accuracy: 0.9883 - 77ms/epoch - 5ms/step\n",
      "Epoch 195/300\n",
      "14/14 - 0s - loss: 0.0614 - accuracy: 0.9836 - 81ms/epoch - 6ms/step\n",
      "Epoch 196/300\n",
      "14/14 - 0s - loss: 0.0608 - accuracy: 0.9883 - 72ms/epoch - 5ms/step\n",
      "Epoch 197/300\n",
      "14/14 - 0s - loss: 0.0601 - accuracy: 0.9859 - 87ms/epoch - 6ms/step\n",
      "Epoch 198/300\n",
      "14/14 - 0s - loss: 0.0578 - accuracy: 0.9836 - 77ms/epoch - 6ms/step\n",
      "Epoch 199/300\n",
      "14/14 - 0s - loss: 0.0580 - accuracy: 0.9859 - 83ms/epoch - 6ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/300\n",
      "14/14 - 0s - loss: 0.0561 - accuracy: 0.9859 - 76ms/epoch - 5ms/step\n",
      "Epoch 201/300\n",
      "14/14 - 0s - loss: 0.0564 - accuracy: 0.9836 - 82ms/epoch - 6ms/step\n",
      "Epoch 202/300\n",
      "14/14 - 0s - loss: 0.0582 - accuracy: 0.9859 - 78ms/epoch - 6ms/step\n",
      "Epoch 203/300\n",
      "14/14 - 0s - loss: 0.0555 - accuracy: 0.9883 - 88ms/epoch - 6ms/step\n",
      "Epoch 204/300\n",
      "14/14 - 0s - loss: 0.0547 - accuracy: 0.9836 - 77ms/epoch - 5ms/step\n",
      "Epoch 205/300\n",
      "14/14 - 0s - loss: 0.0543 - accuracy: 0.9859 - 93ms/epoch - 7ms/step\n",
      "Epoch 206/300\n",
      "14/14 - 0s - loss: 0.0540 - accuracy: 0.9883 - 82ms/epoch - 6ms/step\n",
      "Epoch 207/300\n",
      "14/14 - 0s - loss: 0.0574 - accuracy: 0.9812 - 78ms/epoch - 6ms/step\n",
      "Epoch 208/300\n",
      "14/14 - 0s - loss: 0.0610 - accuracy: 0.9812 - 76ms/epoch - 5ms/step\n",
      "Epoch 209/300\n",
      "14/14 - 0s - loss: 0.0557 - accuracy: 0.9859 - 75ms/epoch - 5ms/step\n",
      "Epoch 210/300\n",
      "14/14 - 0s - loss: 0.0547 - accuracy: 0.9859 - 76ms/epoch - 5ms/step\n",
      "Epoch 211/300\n",
      "14/14 - 0s - loss: 0.0520 - accuracy: 0.9859 - 79ms/epoch - 6ms/step\n",
      "Epoch 212/300\n",
      "14/14 - 0s - loss: 0.0518 - accuracy: 0.9812 - 73ms/epoch - 5ms/step\n",
      "Epoch 213/300\n",
      "14/14 - 0s - loss: 0.0529 - accuracy: 0.9836 - 77ms/epoch - 5ms/step\n",
      "Epoch 214/300\n",
      "14/14 - 0s - loss: 0.0522 - accuracy: 0.9836 - 88ms/epoch - 6ms/step\n",
      "Epoch 215/300\n",
      "14/14 - 0s - loss: 0.0514 - accuracy: 0.9836 - 85ms/epoch - 6ms/step\n",
      "Epoch 216/300\n",
      "14/14 - 0s - loss: 0.0513 - accuracy: 0.9836 - 77ms/epoch - 6ms/step\n",
      "Epoch 217/300\n",
      "14/14 - 0s - loss: 0.0511 - accuracy: 0.9836 - 79ms/epoch - 6ms/step\n",
      "Epoch 218/300\n",
      "14/14 - 0s - loss: 0.0491 - accuracy: 0.9859 - 80ms/epoch - 6ms/step\n",
      "Epoch 219/300\n",
      "14/14 - 0s - loss: 0.0499 - accuracy: 0.9836 - 78ms/epoch - 6ms/step\n",
      "Epoch 220/300\n",
      "14/14 - 0s - loss: 0.0490 - accuracy: 0.9812 - 85ms/epoch - 6ms/step\n",
      "Epoch 221/300\n",
      "14/14 - 0s - loss: 0.0495 - accuracy: 0.9859 - 82ms/epoch - 6ms/step\n",
      "Epoch 222/300\n",
      "14/14 - 0s - loss: 0.0508 - accuracy: 0.9883 - 76ms/epoch - 5ms/step\n",
      "Epoch 223/300\n",
      "14/14 - 0s - loss: 0.0480 - accuracy: 0.9859 - 86ms/epoch - 6ms/step\n",
      "Epoch 224/300\n",
      "14/14 - 0s - loss: 0.0477 - accuracy: 0.9859 - 83ms/epoch - 6ms/step\n",
      "Epoch 225/300\n",
      "14/14 - 0s - loss: 0.0475 - accuracy: 0.9883 - 87ms/epoch - 6ms/step\n",
      "Epoch 226/300\n",
      "14/14 - 0s - loss: 0.0467 - accuracy: 0.9859 - 84ms/epoch - 6ms/step\n",
      "Epoch 227/300\n",
      "14/14 - 0s - loss: 0.0454 - accuracy: 0.9859 - 77ms/epoch - 5ms/step\n",
      "Epoch 228/300\n",
      "14/14 - 0s - loss: 0.0460 - accuracy: 0.9859 - 77ms/epoch - 5ms/step\n",
      "Epoch 229/300\n",
      "14/14 - 0s - loss: 0.0446 - accuracy: 0.9883 - 89ms/epoch - 6ms/step\n",
      "Epoch 230/300\n",
      "14/14 - 0s - loss: 0.0473 - accuracy: 0.9836 - 91ms/epoch - 6ms/step\n",
      "Epoch 231/300\n",
      "14/14 - 0s - loss: 0.0480 - accuracy: 0.9836 - 91ms/epoch - 7ms/step\n",
      "Epoch 232/300\n",
      "14/14 - 0s - loss: 0.0445 - accuracy: 0.9836 - 82ms/epoch - 6ms/step\n",
      "Epoch 233/300\n",
      "14/14 - 0s - loss: 0.0440 - accuracy: 0.9859 - 81ms/epoch - 6ms/step\n",
      "Epoch 234/300\n",
      "14/14 - 0s - loss: 0.0448 - accuracy: 0.9859 - 80ms/epoch - 6ms/step\n",
      "Epoch 235/300\n",
      "14/14 - 0s - loss: 0.0444 - accuracy: 0.9883 - 78ms/epoch - 6ms/step\n",
      "Epoch 236/300\n",
      "14/14 - 0s - loss: 0.0438 - accuracy: 0.9859 - 83ms/epoch - 6ms/step\n",
      "Epoch 237/300\n",
      "14/14 - 0s - loss: 0.0442 - accuracy: 0.9859 - 81ms/epoch - 6ms/step\n",
      "Epoch 238/300\n",
      "14/14 - 0s - loss: 0.0431 - accuracy: 0.9859 - 80ms/epoch - 6ms/step\n",
      "Epoch 239/300\n",
      "14/14 - 0s - loss: 0.0427 - accuracy: 0.9883 - 79ms/epoch - 6ms/step\n",
      "Epoch 240/300\n",
      "14/14 - 0s - loss: 0.0429 - accuracy: 0.9812 - 75ms/epoch - 5ms/step\n",
      "Epoch 241/300\n",
      "14/14 - 0s - loss: 0.0431 - accuracy: 0.9859 - 83ms/epoch - 6ms/step\n",
      "Epoch 242/300\n",
      "14/14 - 0s - loss: 0.0450 - accuracy: 0.9883 - 76ms/epoch - 5ms/step\n",
      "Epoch 243/300\n",
      "14/14 - 0s - loss: 0.0447 - accuracy: 0.9859 - 75ms/epoch - 5ms/step\n",
      "Epoch 244/300\n",
      "14/14 - 0s - loss: 0.0442 - accuracy: 0.9836 - 81ms/epoch - 6ms/step\n",
      "Epoch 245/300\n",
      "14/14 - 0s - loss: 0.0423 - accuracy: 0.9812 - 78ms/epoch - 6ms/step\n",
      "Epoch 246/300\n",
      "14/14 - 0s - loss: 0.0410 - accuracy: 0.9859 - 79ms/epoch - 6ms/step\n",
      "Epoch 247/300\n",
      "14/14 - 0s - loss: 0.0466 - accuracy: 0.9836 - 78ms/epoch - 6ms/step\n",
      "Epoch 248/300\n",
      "14/14 - 0s - loss: 0.0423 - accuracy: 0.9859 - 77ms/epoch - 6ms/step\n",
      "Epoch 249/300\n",
      "14/14 - 0s - loss: 0.0404 - accuracy: 0.9859 - 76ms/epoch - 5ms/step\n",
      "Epoch 250/300\n",
      "14/14 - 0s - loss: 0.0406 - accuracy: 0.9836 - 73ms/epoch - 5ms/step\n",
      "Epoch 251/300\n",
      "14/14 - 0s - loss: 0.0398 - accuracy: 0.9836 - 81ms/epoch - 6ms/step\n",
      "Epoch 252/300\n",
      "14/14 - 0s - loss: 0.0401 - accuracy: 0.9836 - 78ms/epoch - 6ms/step\n",
      "Epoch 253/300\n",
      "14/14 - 0s - loss: 0.0394 - accuracy: 0.9859 - 80ms/epoch - 6ms/step\n",
      "Epoch 254/300\n",
      "14/14 - 0s - loss: 0.0403 - accuracy: 0.9836 - 74ms/epoch - 5ms/step\n",
      "Epoch 255/300\n",
      "14/14 - 0s - loss: 0.0438 - accuracy: 0.9836 - 77ms/epoch - 6ms/step\n",
      "Epoch 256/300\n",
      "14/14 - 0s - loss: 0.0414 - accuracy: 0.9859 - 76ms/epoch - 5ms/step\n",
      "Epoch 257/300\n",
      "14/14 - 0s - loss: 0.0398 - accuracy: 0.9859 - 77ms/epoch - 5ms/step\n",
      "Epoch 258/300\n",
      "14/14 - 0s - loss: 0.0388 - accuracy: 0.9836 - 81ms/epoch - 6ms/step\n",
      "Epoch 259/300\n",
      "14/14 - 0s - loss: 0.0381 - accuracy: 0.9883 - 76ms/epoch - 5ms/step\n",
      "Epoch 260/300\n",
      "14/14 - 0s - loss: 0.0391 - accuracy: 0.9836 - 80ms/epoch - 6ms/step\n",
      "Epoch 261/300\n",
      "14/14 - 0s - loss: 0.0383 - accuracy: 0.9812 - 76ms/epoch - 5ms/step\n",
      "Epoch 262/300\n",
      "14/14 - 0s - loss: 0.0379 - accuracy: 0.9859 - 85ms/epoch - 6ms/step\n",
      "Epoch 263/300\n",
      "14/14 - 0s - loss: 0.0379 - accuracy: 0.9812 - 78ms/epoch - 6ms/step\n",
      "Epoch 264/300\n",
      "14/14 - 0s - loss: 0.0368 - accuracy: 0.9859 - 81ms/epoch - 6ms/step\n",
      "Epoch 265/300\n",
      "14/14 - 0s - loss: 0.0362 - accuracy: 0.9859 - 78ms/epoch - 6ms/step\n",
      "Epoch 266/300\n",
      "14/14 - 0s - loss: 0.0375 - accuracy: 0.9859 - 79ms/epoch - 6ms/step\n",
      "Epoch 267/300\n",
      "14/14 - 0s - loss: 0.0368 - accuracy: 0.9836 - 76ms/epoch - 5ms/step\n",
      "Epoch 268/300\n",
      "14/14 - 0s - loss: 0.0385 - accuracy: 0.9883 - 78ms/epoch - 6ms/step\n",
      "Epoch 269/300\n",
      "14/14 - 0s - loss: 0.0376 - accuracy: 0.9859 - 80ms/epoch - 6ms/step\n",
      "Epoch 270/300\n",
      "14/14 - 0s - loss: 0.0373 - accuracy: 0.9883 - 77ms/epoch - 6ms/step\n",
      "Epoch 271/300\n",
      "14/14 - 0s - loss: 0.0377 - accuracy: 0.9859 - 81ms/epoch - 6ms/step\n",
      "Epoch 272/300\n",
      "14/14 - 0s - loss: 0.0402 - accuracy: 0.9883 - 77ms/epoch - 6ms/step\n",
      "Epoch 273/300\n",
      "14/14 - 0s - loss: 0.0392 - accuracy: 0.9859 - 81ms/epoch - 6ms/step\n",
      "Epoch 274/300\n",
      "14/14 - 0s - loss: 0.0377 - accuracy: 0.9836 - 78ms/epoch - 6ms/step\n",
      "Epoch 275/300\n",
      "14/14 - 0s - loss: 0.0364 - accuracy: 0.9836 - 76ms/epoch - 5ms/step\n",
      "Epoch 276/300\n",
      "14/14 - 0s - loss: 0.0355 - accuracy: 0.9836 - 82ms/epoch - 6ms/step\n",
      "Epoch 277/300\n",
      "14/14 - 0s - loss: 0.0359 - accuracy: 0.9883 - 84ms/epoch - 6ms/step\n",
      "Epoch 278/300\n",
      "14/14 - 0s - loss: 0.0365 - accuracy: 0.9883 - 85ms/epoch - 6ms/step\n",
      "Epoch 279/300\n",
      "14/14 - 0s - loss: 0.0355 - accuracy: 0.9883 - 79ms/epoch - 6ms/step\n",
      "Epoch 280/300\n",
      "14/14 - 0s - loss: 0.0353 - accuracy: 0.9883 - 80ms/epoch - 6ms/step\n",
      "Epoch 281/300\n",
      "14/14 - 0s - loss: 0.0362 - accuracy: 0.9883 - 76ms/epoch - 5ms/step\n",
      "Epoch 282/300\n",
      "14/14 - 0s - loss: 0.0352 - accuracy: 0.9859 - 77ms/epoch - 5ms/step\n",
      "Epoch 283/300\n",
      "14/14 - 0s - loss: 0.0345 - accuracy: 0.9836 - 75ms/epoch - 5ms/step\n",
      "Epoch 284/300\n",
      "14/14 - 0s - loss: 0.0344 - accuracy: 0.9859 - 79ms/epoch - 6ms/step\n",
      "Epoch 285/300\n",
      "14/14 - 0s - loss: 0.0342 - accuracy: 0.9812 - 80ms/epoch - 6ms/step\n",
      "Epoch 286/300\n",
      "14/14 - 0s - loss: 0.0341 - accuracy: 0.9859 - 85ms/epoch - 6ms/step\n",
      "Epoch 287/300\n",
      "14/14 - 0s - loss: 0.0361 - accuracy: 0.9859 - 88ms/epoch - 6ms/step\n",
      "Epoch 288/300\n",
      "14/14 - 0s - loss: 0.0370 - accuracy: 0.9812 - 88ms/epoch - 6ms/step\n",
      "Epoch 289/300\n",
      "14/14 - 0s - loss: 0.0341 - accuracy: 0.9859 - 82ms/epoch - 6ms/step\n",
      "Epoch 290/300\n",
      "14/14 - 0s - loss: 0.0346 - accuracy: 0.9836 - 80ms/epoch - 6ms/step\n",
      "Epoch 291/300\n",
      "14/14 - 0s - loss: 0.0332 - accuracy: 0.9859 - 78ms/epoch - 6ms/step\n",
      "Epoch 292/300\n",
      "14/14 - 0s - loss: 0.0334 - accuracy: 0.9812 - 81ms/epoch - 6ms/step\n",
      "Epoch 293/300\n",
      "14/14 - 0s - loss: 0.0333 - accuracy: 0.9859 - 77ms/epoch - 6ms/step\n",
      "Epoch 294/300\n",
      "14/14 - 0s - loss: 0.0330 - accuracy: 0.9836 - 78ms/epoch - 6ms/step\n",
      "Epoch 295/300\n",
      "14/14 - 0s - loss: 0.0324 - accuracy: 0.9836 - 80ms/epoch - 6ms/step\n",
      "Epoch 296/300\n",
      "14/14 - 0s - loss: 0.0323 - accuracy: 0.9836 - 75ms/epoch - 5ms/step\n",
      "Epoch 297/300\n",
      "14/14 - 0s - loss: 0.0336 - accuracy: 0.9836 - 76ms/epoch - 5ms/step\n",
      "Epoch 298/300\n",
      "14/14 - 0s - loss: 0.0324 - accuracy: 0.9836 - 76ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299/300\n",
      "14/14 - 0s - loss: 0.0322 - accuracy: 0.9859 - 87ms/epoch - 6ms/step\n",
      "Epoch 300/300\n",
      "14/14 - 0s - loss: 0.0335 - accuracy: 0.9812 - 85ms/epoch - 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2caffb79e40>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "hidden_units = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_units, input_shape = (X_data_one_hot.shape[1], X_data_one_hot.shape[2])))\n",
    "model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(X_data_one_hot, y_data_one_hot, epochs = 300, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b009f56",
   "metadata": {},
   "source": [
    "문장을 생성하는 함수 sentence_generation을 만들어서 문장을 생성해보자, 해당 함수는 문자열을 입력하면 해당 문자열로부터 다음 문자를 예측하는 것을 반복하여 최종적으로 문장을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0dcc174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, char_to_index, seq_length, seed_text, n):\n",
    "\n",
    "    # 초기 시퀀스\n",
    "    init_text = seed_text\n",
    "    sentence = ''\n",
    "\n",
    "    # 다음 문자 예측은 총 n번만 반복.\n",
    "    for _ in range(n):\n",
    "        encoded = [char_to_index[char] for char in seed_text] # 현재 시퀀스에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, padding='pre') # 데이터에 대한 패딩\n",
    "        encoded = to_categorical(encoded, num_classes=len(char_to_index))\n",
    "\n",
    "        # 입력한 X(현재 시퀀스)에 대해서 y를 예측하고 y(예측한 문자)를 result에 저장.\n",
    "        result = model.predict(encoded, verbose=0)\n",
    "        result = np.argmax(result, axis=1)\n",
    "\n",
    "        for char, index in char_to_index.items():\n",
    "            if index == result:\n",
    "                break\n",
    "\n",
    "        # 현재 시퀀스 + 예측 문자를 현재 시퀀스로 변경\n",
    "        seed_text = seed_text + char\n",
    "\n",
    "        # 예측 문자를 문장에 저장\n",
    "        sentence = sentence + char\n",
    "\n",
    "    # n번의 다음 문자 예측이 끝나면 최종 완성된 문장을 리턴.\n",
    "    sentence = init_text + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "915c3722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I get on with life as a programmer, I like to contemplate beer. But when I start to daydre\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, char_to_index, 10, 'I get on w', 80)) # 80번 반복"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
