{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e1d699",
   "metadata": {},
   "source": [
    "4-2. Bag of Words(BoW)\n",
    "* BoW - 단어들의 순서를 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4575195",
   "metadata": {},
   "source": [
    "문서1 : 정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a9bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def build_bag_of_words(document):\n",
    "  # 온점 제거 및 형태소 분석\n",
    "  document = document.replace('.', '')\n",
    "  tokenized_document = okt.morphs(document)\n",
    "\n",
    "  word_to_index = {}\n",
    "  bow = []\n",
    "\n",
    "  for word in tokenized_document:  \n",
    "    if word not in word_to_index.keys():\n",
    "      word_to_index[word] = len(word_to_index)  \n",
    "      # BoW에 전부 기본값 1을 넣는다.\n",
    "      bow.insert(len(word_to_index) - 1, 1)\n",
    "    else:\n",
    "      # 재등장하는 단어의 인덱스\n",
    "      index = word_to_index.get(word)\n",
    "      # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더한다.\n",
    "      bow[index] = bow[index] + 1\n",
    "\n",
    "  return word_to_index, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5468b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
      "bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\n",
    "vocab, bow = build_bag_of_words(doc1)\n",
    "print('vocabulary :', vocab)\n",
    "print('bag of words vector :', bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9235379",
   "metadata": {},
   "source": [
    "문서 2 : 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cca839f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary : {'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
      "bag of words vector : [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc2 = '소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.'\n",
    "\n",
    "vocab, bow = build_bag_of_words(doc2)\n",
    "print('vocabulary :', vocab)\n",
    "print('bag of words vector :', bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91144b",
   "metadata": {},
   "source": [
    "문서3: 정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a50e406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\n",
      "bag of words vector : [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc3 = doc1 + ' ' + doc2\n",
    "vocab, bow = build_bag_of_words(doc3)\n",
    "print('vocabulary :', vocab)\n",
    "print('bag of words vector :', bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54584536",
   "metadata": {},
   "source": [
    "bow는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이므로 주로 어떤 단어가 얼마나 등장했는지를 기준으로 \\\n",
    "문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰인다. \\\n",
    "즉, 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 주로 쓰인다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf32ae3",
   "metadata": {},
   "source": [
    "CountVectorizer 클래스로 BoW 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40f6ee31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 2 1 2 1]]\n",
      "vocabulary : {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print('bag of words vector :', vector.fit_transform(corpus).toarray())\n",
    "\n",
    "# 각 단어의 인덱스가 어떻게 부여되었는지를 출력\n",
    "print('vocabulary :', vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8b45f",
   "metadata": {},
   "source": [
    "알파벳 I는 BoW를 만드는 과정에서 사라졌는데, 이는 CountVectorizer가 기본적으로 길이가 2이상인 문자에 대해서만 토큰으로 인식하기 때문이다 \\\n",
    "주의할 것은 CountVectorizer는 단지 띄어쓰기만을 기준으로 단어를 자르는 낮은 수준의 토큰화를 진행하고 BoW를 만든다는 점인데, \\\n",
    "영어의 경우에는 상관없지만 한국어에 CountVectorizer를 적용하면, 조사 등의 이유로 제대로 BoW가 만들어지지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c6f7e3",
   "metadata": {},
   "source": [
    "불용어를 제거한 BoW 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca4d5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ccef808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1 1 1]]\n",
      "vocabulary : {'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"]) # 사용자 정의 불용어 사용\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
    "print('vocabulary :',vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dee4d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1]]\n",
      "vocabulary : {'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=\"english\") # CounterVectorizer에서 제공하는 자체 불용어 사용\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
    "print('vocabulary :',vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b295ad80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1 1]]\n",
      "vocabulary : {'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "stop_words = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words = stop_words)\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
    "print('vocabulary :',vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10efb81",
   "metadata": {},
   "source": [
    "CountVectorizer에서 제공된 불용어는 everything에서 every를 불용어 처리해서 thing만 남겼는데 \\\n",
    "NLTK에서 제공된 불용어를 사용했을때는 everything 그 자체로 사용되어서 성능이 nltk쪽이 조금 더 좋은것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25508c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
