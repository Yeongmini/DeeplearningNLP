{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d494da7",
   "metadata": {},
   "source": [
    "7-7. 기울기 소실(Gradient Vanishing)과 폭주(Exploding)\n",
    "* 기울기 소실 - 역전파 과정에서 입력층으로 갈 수록 기울기가 점차적으로 작아지는 현상\n",
    "* 기울기 폭주 - 기울기가 점차 커지더니 가중치들이 비정상적으로 큰 값이 되면서 발산하는 현상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6661e2",
   "metadata": {},
   "source": [
    "1. ReLU와 ReLU의 변형들 \\\n",
    "시그모이드 함수를 사용하면 입력의 절대값이 큰 경우 시그모이드 함수의 출력값이 0 또는 1에 수렴하면서 기울기가 0에 가까워진다.(기울기 소실 발생)\\\n",
    "기울기 소실을 완화하는 방법은 은닉층의 활성화 함수로 ReLU나 ReLU의 변형인 leaky ReLU를 사용하는것이다.\n",
    "* 은닉층에서는 시그모이드 함수를 사용하면 안된다.\n",
    "* Leaky ReLU를 사용하면 모든 입력값에 대해서 기울기가 0에 수렴하지 않아 죽은 ReLU 문제를 해결한다.\n",
    "* 은닉층에서는 ReLU나 Leaky ReLU와 같은 ReLU 함수의 변형들을 사용해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb18ae",
   "metadata": {},
   "source": [
    "2. 그래디언트 클리핑(Gradient Clipping)\n",
    "* 말그대로 기울기 값을 자르는 것을 의미한다. 임계값을 넘지 않도록 값을 자른다. 즉, 임계치만큼 크기를 감소시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스에서 사용하는 그래디언트 클리핑 예시\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "Adam = optimizers.Adam(lr = 0.0001, clipnorm = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b97ca",
   "metadata": {},
   "source": [
    "3. 가중치 초기화(Weight initialization)\n",
    "* 같은 모델을 훈련시키더라도 가중치가 초기에 어떤 값을 가졌느냐에 따라서 모델의 훈련 결과가 달라지기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fdb8e6",
   "metadata": {},
   "source": [
    "1) 세이비어 초기화(Xavier Initialization)\n",
    "* 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목을 받거나 다른 층이 뒤쳐지는 것을 막는다.\n",
    "* 시그모이드 함수나 하이퍼볼릭 탄젠트 함수와 같은 활성화 함수와 사용할 경우에 좋은 성능을 보이지만, ReLU와 함께 사용할 경우에는 성능이 좋지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d7312",
   "metadata": {},
   "source": [
    "2) He 초기화(He initialization)\n",
    "* ReLU 계열 함수를 사용할 경우에는 He 초기화 방법이 효율적이다.\n",
    "* ReLU + He 초기화 방법이 좀 더 보편적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07699554",
   "metadata": {},
   "source": [
    "4. 배치 정규화(Batch Normalization)\n",
    "* 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec880e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7ebc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
