{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef4b5017",
   "metadata": {},
   "source": [
    "9-6 패스트 텍스트(FastText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd757d0",
   "metadata": {},
   "source": [
    "Word2Vec는 단어를 쪼개질 수 없는 단위로 생각한다면, FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주한다.\\\n",
    "내부단어 즉, 서브워드(subword)를 고려하여 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa796ee",
   "metadata": {},
   "source": [
    "1. 내부 단어의 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feff6cf",
   "metadata": {},
   "source": [
    "FastText에서는 각 단어는 글자 단위 n-gram의 구성으로 취급한다. 만약 n을 3으로 잡은 tri-gram의 경우\\\n",
    "apple은 app,ppl,ple로 분리하고 이를 벡터로 만든다. 정확히는 시작과 끝을 의미하는 <,>를 도입하여 5개 내부 단어 토큰을 벡터로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 3인 경우\n",
    "<ap, app, ppl, ple, le>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9875be49",
   "metadata": {},
   "source": [
    "그리고 여기에 추가적으로 하나를 더 벡터화하는데, 이는 기존 단어에 < 와 > 를 붙인 토큰이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가 토큰\n",
    "<apple>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94685f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#즉, n = 3인 경우에 FastText는 단어 apple에 대해서 6개의 토큰을 벡터화 한다\n",
    "<ap, app, ppl, ple, le>, <apple>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddcd6a6",
   "metadata": {},
   "source": [
    "그런데 실제로 사용할 때에는 n의 최솟값과 최대값으로 설정할 수 있는데, 기본값으로는 각각 3과 6으로 설정되어져 있다.\\\n",
    "즉 최솟값 = 3, 최대값 = 6인 경우라면 단어 apple에 대해서 FastText는 아래 내부 단어들을 벡터화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 3 ~ 6 인 경우\n",
    "<ap, app, ppl, ple, le>, <app, appl, pple, ple>, <appl, pple>, ..., <apple>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5232a33",
   "metadata": {},
   "source": [
    "내부 단어들을 벡터화한다는 의미는 저 단어들에 대해서 Word2Vec을 수행한다는 의미이다.\\\n",
    "내부 단어들의 벡터값을 얻었다면, 단어 apple의 벡터값은 저 위 벡터값들의 총 합으로 구성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = <ap + app + ppl + ppl + le> + <app + appl + pple + ple> + <appl + pple> + , ..., +<apple>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd7d03",
   "metadata": {},
   "source": [
    "2. 모르는 단어(Out of Vocabulary, OOV)에 대한 대응"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b534c",
   "metadata": {},
   "source": [
    "FastText의 인공 신경망을 학습한 후 데이터 셋의 모든 단어의 각 n-gram에 대해서 임베딩 되는데, 이렇게 되면 데이터 셋만 충분하면 위와 같은 내부 단어를 통해 모르는 단어(OOV)에 대해서도 다른 단어와의 유사도를 계산할수 있다.\\\n",
    "ex) FastText에서 birthplace(출생지)란 단어가 학습되지 않은 상태라고 가정하자. 하지만 다른 단어에서 birth와 place라는 내부 단어가 있었다면, FastText는 birthplace라는 벡터를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d456c",
   "metadata": {},
   "source": [
    "3. 단어 집합 내 빈도 수가 적었던 단어(Rare Word)에 대한 대응"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ffe499",
   "metadata": {},
   "source": [
    "Word2Vec의 경우에는 등장 빈도수가 적은 단어(rare word)에 대해서는 임베딩 정확도가 높지 않다는 단점이 존재.\\\n",
    "FastText의 경우 단어가 희귀 단어라도 그 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우라면, Word2Vec과 비교했을 때 비교적 높은 임베딩 벡터값을 얻는다.\\\n",
    "실제 데이터들에는 오타가 섞여있는 경우가 많고 이로인해 등장 빈도수가 적어져 희귀 단어 처리가 되는 경우가 많은데, FastText는 이에 대해서도 일정 수준의 성능을 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c1a9d7",
   "metadata": {},
   "source": [
    "4. 실습으로 비교하는 Word2Vec Vs. FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97182342",
   "metadata": {},
   "source": [
    "1) Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5393683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_wv = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc22e3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'electrofishing' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel_wv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43melectrofishing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_env\\lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_env\\lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'electrofishing' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "model_wv.most_similar(\"electrofishing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773ec9f9",
   "metadata": {},
   "source": [
    "단어 집합에 electrofishing이 존재하지 않아서 에러가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f56f6",
   "metadata": {},
   "source": [
    "2) FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ecac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4887850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower()) # a-z , 0-9 제와 나머지 공백처리인듯\n",
    "     normalized_text.append(tokens)\n",
    "\n",
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]\n",
    "\n",
    "model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e6a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"electrofishing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45ba781",
   "metadata": {},
   "source": [
    "5. 한국어에서의 FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978d0831",
   "metadata": {},
   "source": [
    "(1) 음절 단위"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3201040",
   "metadata": {},
   "source": [
    "예를 들어서 음절 단위의 임베딩의 경우에 n=3 일때 '자연어처리'라는 단어에 대해 n-gram을 만들어보면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76456136",
   "metadata": {},
   "outputs": [],
   "source": [
    "<자연, 자연어, 연어처, 어처리, 처리>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b09d6",
   "metadata": {},
   "source": [
    "(2) 자모 단위"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66107482",
   "metadata": {},
   "source": [
    "음절 단위가 아니라, 자모 단위로 가게 되면 오타나 노이즈 측면에서 더 강한 임베딩을 기대해볼 수 있다.\\\n",
    "예를 들어 '자연어처리'라는 단어에 대해서 초성,중성,종성을 분리하고 만약 종성이 존재하지 않는다면 '-'라는 토큰을 사용한다고 가정하면 아래와 같이 분리된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63db23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ㅈ ㅏ _ ㅇ ㅕ ㄴ ㅇ ㅓ _ ㅊ ㅓ _ ㄹ ㅣ _"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
