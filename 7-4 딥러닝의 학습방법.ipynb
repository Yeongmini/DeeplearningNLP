{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008afde6",
   "metadata": {},
   "source": [
    "7-4. 딥 러닝의 학습 방법\n",
    "* 손실 함수, 옵티마이저, 에포크"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d82637",
   "metadata": {},
   "source": [
    "1. 손실 함수\n",
    "* 손실 함수는 실제값과 예측값의 차이를 수치화해주는 함수이다. 오차가 클 수록 손실 함수의 값은 크고 오차가 작을수록 손실 함수의 값은 작아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4850b15",
   "metadata": {},
   "source": [
    "1) MSE(Mean Squared Error, MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a5b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd909f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss=tf.keras.losses.MeanSquaredError(), metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f935093",
   "metadata": {},
   "source": [
    "위의 두 코드는 같은 결과값을 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd420817",
   "metadata": {},
   "source": [
    "딥 러닝 자연어 처리는 대부분 분류 문제이므로 평균 제곱 오차보다는 크로스 엔트로피 함수를 주로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31511e47",
   "metadata": {},
   "source": [
    "2) 이진 크로스 엔트로피(Binary Cross-Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc']) # 출력층 에서 시그모이드 함수를 사용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af32896",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.binary_crossentropy(), metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e01b8aa",
   "metadata": {},
   "source": [
    "3) 카테고리칼 크로스 엔트로피(Categorical Cross-Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98bc19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc']) # 출력층 에서 소프트 맥스 함수를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae634f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.categorical_crossentropy, metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db1ae7",
   "metadata": {},
   "source": [
    "만약, 레이블에서 원-핫 인코딩 과정을 생략하고, 정수값을 가진 레이블에 대해서 다중 클래스 분류를 수행하고 싶으면 다음과 같이 \\\n",
    "'sparse_categorical_crossentropy'를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf1eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1938e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.sparse_categorical_crossentropy, metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d64b677",
   "metadata": {},
   "source": [
    "2. 배치 크기(Batch size)에 따른 경사 하강법\n",
    "* 배치(Batch) = 가중치 등의 매개 변수의 값을 조정하기 위해 사용하는 데이터의 양, 전체 데이터를 가지고 매개 변수의 값을 조정할 수 있고, 정해준 양의 데이터만 가지고도 매개 변수의 값을 조정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6740819d",
   "metadata": {},
   "source": [
    "1) 배치 경사 하강법(Batch Gradient Descent)\n",
    "* 가장 기본적인 하강법으로, 오차를 구할 때 전체 데이터를 고려한다. 배치 경사 하강법은 한 번의 에포크에 모든 매개변수 업데이트를 단 한 번 수행한다. 이로 인해 시간이 오래 걸리며, 메모리를 크게 요구한다는 단점이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b45ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size = len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be2e92",
   "metadata": {},
   "source": [
    "2) 배치 크기가 1인 확률적 경사 하강법(Stochastic Gradient Descent, SGD)\n",
    "* 기존의 배치 경사 하강법은 시간이 너무 오래 걸린다는 단점이 존재, 배치 크기가 1인 확률적 경사 하강법은 매개변수 값을 조정 시 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법이다. 따라서 더 적은 데이터를 사용하므로 더 빠르게 계산이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458c3bd",
   "metadata": {},
   "source": [
    "확률적 경사 하강법은 매개변수의 변경폭이 불안정하고, 때로는 배치 경사 하강법보다 정확도가 낮을 수 있지만 하나의 데이터에 대해서만 메모리에 저장하면 되므로 자원이 적은 컴퓨터에서도 쉽게 사용가능 하다는 장점이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2398b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59042093",
   "metadata": {},
   "source": [
    "3) 미니 배치 경사 하강법(Mini-Batch Gradient Descent)\n",
    "* 전체 데이터도, 1개의 데이터도 아닐 때, 배치 크기를 지정하여 해당 데이터 개수만큼에 대해서 계산하여 매개 변수 값을 조정하는 하강법이다. 미니 배치 경사 하강법은 전체 데이터를 계산하는것 보다 빠르며, SGD보다 안정적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e82db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size = 128) # batch_size 값의 default 값은 32이다(2의 5제곱)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73b6c4",
   "metadata": {},
   "source": [
    "배치 크기는 일반적으로 2의 n제곱에 해당하는 숫자로 선택하는 것이 보편적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796cd02d",
   "metadata": {},
   "source": [
    "3. 옵티마이저(Optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59050cb",
   "metadata": {},
   "source": [
    "1) 모멘텀(Momentum)\n",
    "* 모멘텀은 관성이라는 물리학의 법칙을 응용한 방법으로, 모멘텀 경사 하강법에 관성을 더해준다. 모멘텀은 경사 하강법에서 계산된 접선의 기울기에 한 시점 전의 접선의 기울기값을 일정한 비율만큼 반영한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a74ef",
   "metadata": {},
   "source": [
    "로컬 미니멈에 도달했을 때 글로벌 미니멈으로 잘못 인식하여 탈출하지 못하였을 때, 모멘텀을 사용하면 값이 조절되면서 현재의 로컬 미니멈에서 탈출하고 글로벌 미니멈 내지는 더 낮은 로컬 미니멈으로 갈 수 있는 효과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.optimizers.SGD(lr = 0.01, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97752f20",
   "metadata": {},
   "source": [
    "2) 아다그라드(Adagrad)\n",
    "* 보통은 모든 매개변수에 동일한 학습률(lr)을 적용하지만 이는 비효율적이다. 아다그라드는 각 매개 변수에 서로 다른 학습률을 적용시킨다. 이때 변화가 많은 매개변수는 학습률이 작게 설정되고, 변화가 적은 매개변수는 학습률을 높게 설정시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac832ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.optimizers.Adagrad(lr = 0.01, epsilon = 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fe0956",
   "metadata": {},
   "source": [
    "3) 알엠에스프롭(RMSprop)\n",
    "* 아다그라드는 학습을 계속 진행한 경우에는, 나중에 가서 학습률이 지나치게 떨어지는 단점이 존재하는데, 이를 다른 수식으로 대체한 것이 알엠에스프롭이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.optimizers.RMSprop(lr = 0.001, rho = 0.9, epsilon = 1e-06)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cbe3b3",
   "metadata": {},
   "source": [
    "4) 아담(adam)\n",
    "* 아담은 알엠에스프롭(RMSprop)과 모멘텀(Momentum) 두 가지를 합친 듯한 방법으로, 방향과 학습률 두 가지를 모두 잡기 위한 방법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759efe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51964e9f",
   "metadata": {},
   "source": [
    "사용방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad = False)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc752fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbc859",
   "metadata": {},
   "source": [
    "다른 옵티마이저들도 마찬가지로 optimizer = 'sgd', optimizer = 'rmsprop'와 같이 각 옵티마이저를 문자열로 호출할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3ab6a",
   "metadata": {},
   "source": [
    "4. 역전파(BackPropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae84fec",
   "metadata": {},
   "source": [
    "5. 에포크와 배치 크기와 이터레이션(Epochs and Batch size and lteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23edae9",
   "metadata": {},
   "source": [
    "1) 에포크(Epoch)\n",
    "* 에포크란 인공 신경망에서 전체 데이터에 대해서 순전파와 역전파가 끝난 상태를 말한다. 만약 에포크가 50이라고 하면, 전체 데이터 단위로는 총 50번 학습한것이다. 이 에포크 횟수가 지나치거나 너무 적으면 앞서 배운 과적합과 과소적합이 발생할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5628e4",
   "metadata": {},
   "source": [
    "2) 배치 크기(Batch size)\n",
    "* 배치 크기는 몇 개의 데이터 단위로 매개변수를 업데이트 하는지를 말한다. \\\n",
    "ex) 2,000 문제가 수록되어있는 문제지의 문제를 200개 단위로 풀고 채점한다면 이때 배치 크기는 200이 된다. \\\n",
    "여기서 주목할 점은 배치 크기와 배치 수는 다른 개념인데, 전체 데이터가 2,000일 때 배치 크기를 200으로 준다면 배치의 수는 10이다. \\\n",
    "이는 에포크에서 배치 크기를 나눠준 값(2,000/200 = 10)이기도 하다. 이때 배치의 수를 이터레이션이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1603c6b2",
   "metadata": {},
   "source": [
    "3) 이터레이션(Iteration) 또는 스텝(Step)\n",
    "* 이터레이션이란 한 번의 에포크를 끝내기 위해서 필요한 배치의 수를 말한다. 전체 데이터가 2,000일때 배치 크기를 200으로 한다면 이터레이션의 수는 총 10이다. 이는 한 번의 에포크 당 매개변수 업데이트가 10번 이루어지는것을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427fa63b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
